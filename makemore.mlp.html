<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Implementation of mlp model using Pytorch">

<title>minion - mlp</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="minion - mlp">
<meta property="og:description" content="Implementation of mlp model using Pytorch">
<meta property="og:site_name" content="minion">
<meta name="twitter:title" content="minion - mlp">
<meta name="twitter:description" content="Implementation of mlp model using Pytorch">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">minion</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./makemore.mlp.html">mlp</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">minion</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./core.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">core</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./utils.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">utils</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">nn</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./makemore.bigram.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">bigrams</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./makemore.mlp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">mlp</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./makemore.activations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">activations</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#motivations" id="toc-motivations" class="nav-link active" data-scroll-target="#motivations">Motivations</a></li>
  <li><a href="#read-the-dataset" id="toc-read-the-dataset" class="nav-link" data-scroll-target="#read-the-dataset">Read the dataset</a>
  <ul class="collapse">
  <li><a href="#build_xy" id="toc-build_xy" class="nav-link" data-scroll-target="#build_xy">build_XY</a></li>
  </ul></li>
  <li><a href="#implementing-neural-network-step-by-step" id="toc-implementing-neural-network-step-by-step" class="nav-link" data-scroll-target="#implementing-neural-network-step-by-step">Implementing Neural Network Step by Step</a>
  <ul class="collapse">
  <li><a href="#building-embedding-lookup-table-c" id="toc-building-embedding-lookup-table-c" class="nav-link" data-scroll-target="#building-embedding-lookup-table-c">Building embedding lookup table C</a></li>
  <li><a href="#sidenote-on-pytorch-indexing" id="toc-sidenote-on-pytorch-indexing" class="nav-link" data-scroll-target="#sidenote-on-pytorch-indexing">Sidenote on Pytorch indexing</a></li>
  <li><a href="#implementing-embedding-lookup-table" id="toc-implementing-embedding-lookup-table" class="nav-link" data-scroll-target="#implementing-embedding-lookup-table">Implementing embedding lookup table</a></li>
  <li><a href="#implementing-the-hidden-layer-internals-of-torch.tensor-storage-views" id="toc-implementing-the-hidden-layer-internals-of-torch.tensor-storage-views" class="nav-link" data-scroll-target="#implementing-the-hidden-layer-internals-of-torch.tensor-storage-views">Implementing the hidden layer + internals of torch.Tensor: storage, views</a></li>
  <li><a href="#final-layer" id="toc-final-layer" class="nav-link" data-scroll-target="#final-layer">Final layer</a></li>
  </ul></li>
  <li><a href="#refactoring" id="toc-refactoring" class="nav-link" data-scroll-target="#refactoring">Refactoring</a>
  <ul class="collapse">
  <li><a href="#model-class" id="toc-model-class" class="nav-link" data-scroll-target="#model-class">Model Class</a></li>
  <li><a href="#model" id="toc-model" class="nav-link" data-scroll-target="#model">Model</a></li>
  <li><a href="#model-loss" id="toc-model-loss" class="nav-link" data-scroll-target="#model-loss">Model loss</a></li>
  <li><a href="#softmax" id="toc-softmax" class="nav-link" data-scroll-target="#softmax">softmax</a></li>
  <li><a href="#nll" id="toc-nll" class="nav-link" data-scroll-target="#nll">nll</a></li>
  <li><a href="#forward-pass-and-update" id="toc-forward-pass-and-update" class="nav-link" data-scroll-target="#forward-pass-and-update">Forward Pass and Update</a></li>
  <li><a href="#minibatch-construct" id="toc-minibatch-construct" class="nav-link" data-scroll-target="#minibatch-construct">Minibatch Construct</a></li>
  <li><a href="#plot_embeddings" id="toc-plot_embeddings" class="nav-link" data-scroll-target="#plot_embeddings">plot_embeddings</a></li>
  <li><a href="#training-function" id="toc-training-function" class="nav-link" data-scroll-target="#training-function">Training Function</a></li>
  <li><a href="#train" id="toc-train" class="nav-link" data-scroll-target="#train">train</a></li>
  <li><a href="#training-with-minibatch" id="toc-training-with-minibatch" class="nav-link" data-scroll-target="#training-with-minibatch">Training with minibatch</a></li>
  <li><a href="#implementing-word-generator" id="toc-implementing-word-generator" class="nav-link" data-scroll-target="#implementing-word-generator">Implementing Word Generator</a></li>
  <li><a href="#gen_word_nn" id="toc-gen_word_nn" class="nav-link" data-scroll-target="#gen_word_nn">gen_word_nn</a></li>
  </ul></li>
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning">Hyperparameter tuning</a>
  <ul class="collapse">
  <li><a href="#how-do-we-determine-the-learning-rate-and-gain-confidence-that-we-are-stepping-the-right-speed" id="toc-how-do-we-determine-the-learning-rate-and-gain-confidence-that-we-are-stepping-the-right-speed" class="nav-link" data-scroll-target="#how-do-we-determine-the-learning-rate-and-gain-confidence-that-we-are-stepping-the-right-speed">How do we determine the learning rate and gain confidence that we are stepping the right speed?</a></li>
  <li><a href="#lr_scheduler" id="toc-lr_scheduler" class="nav-link" data-scroll-target="#lr_scheduler">lr_scheduler</a></li>
  </ul></li>
  <li><a href="#review" id="toc-review" class="nav-link" data-scroll-target="#review">Review</a>
  <ul class="collapse">
  <li><a href="#why-should-you-prefer-f.cross_entropy-over-rolling-your-own-implementation-like-nll-above" id="toc-why-should-you-prefer-f.cross_entropy-over-rolling-your-own-implementation-like-nll-above" class="nav-link" data-scroll-target="#why-should-you-prefer-f.cross_entropy-over-rolling-your-own-implementation-like-nll-above">Why should you prefer <code>F.cross_entropy</code> over rolling your own implementation like <code>nll</code> above?</a></li>
  <li><a href="#why-is-minibatch-useful-how-do-you-do-it" id="toc-why-is-minibatch-useful-how-do-you-do-it" class="nav-link" data-scroll-target="#why-is-minibatch-useful-how-do-you-do-it">Why is minibatch useful? How do you do it?</a></li>
  <li><a href="#why-do-we-not-achieve-exactly-zero-loss-in-above-operation-with-32-size-5-words" id="toc-why-do-we-not-achieve-exactly-zero-loss-in-above-operation-with-32-size-5-words" class="nav-link" data-scroll-target="#why-do-we-not-achieve-exactly-zero-loss-in-above-operation-with-32-size-5-words">Why do we not achieve exactly zero loss in above operation with 32 size ( 5 words)?</a></li>
  <li><a href="#why-do-we-need-to-split-dataset" id="toc-why-do-we-need-to-split-dataset" class="nav-link" data-scroll-target="#why-do-we-need-to-split-dataset">Why do we need to split dataset?</a></li>
  <li><a href="#increasing-number-of-neurons" id="toc-increasing-number-of-neurons" class="nav-link" data-scroll-target="#increasing-number-of-neurons">Increasing number of neurons</a></li>
  <li><a href="#increasing-embedding-size" id="toc-increasing-embedding-size" class="nav-link" data-scroll-target="#increasing-embedding-size">Increasing Embedding size</a></li>
  <li><a href="#additional-hyperparameter-tuning" id="toc-additional-hyperparameter-tuning" class="nav-link" data-scroll-target="#additional-hyperparameter-tuning">Additional Hyperparameter Tuning</a></li>
  </ul></li>
  <li><a href="#todo" id="toc-todo" class="nav-link" data-scroll-target="#todo">Todo</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/Rahuketu86/minion/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">mlp</h1>
</div>

<div>
  <div class="description">
    Implementation of mlp model using Pytorch
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="motivations" class="level2">
<h2 class="anchored" data-anchor-id="motivations">Motivations</h2>
<ul>
<li>Classical bigram model based on count is okay for bigram -2 characters</li>
<li>However as we increase context windows, it quickly become out of hand
<ul>
<li>P(1 char|Given 1 prev char) -&gt; 27 rows in Prob-table</li>
<li>P(1 char|Given 2 prev char) -&gt; 27*27 = 729 rows in Prob-table</li>
<li>P(1 char|Given 3 prev char) -&gt; 27 * 27 * 27 = 19683 ~ 20K rows in Prob-table or 20K possibilities of context</li>
</ul></li>
<li>Solution : Yoshua Bengio - Neural probabilistic language model
<ul>
<li>Word level language model (17K words, feature vector=30 - embedding–&gt; Every word is embedded into 30 dimensional space). With 17K words, this space is very crowded - lot of points for 30 dimensional space</li>
<li>We will randomly initialize feature vectors per word</li>
<li>Then tune embeddings of the word using back propogation. During the course of training we can imagine that the words will be moving around in this space.</li>
<li>Intuition - words with very similar meanings or are synonyms of each other may end up in very similar part of this space. Words that mean very different things may go somewhere else in the space.</li>
</ul>
Modelling approach
<ul>
<li>MLP to predict the next word given the previous word</li>
<li>To train the network weights - maximization of log likelihood.</li>
<li><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-3-2-image.png" class="img-fluid figure-img"></p>
<figcaption>image.png</figcaption>
</figure>
</div></li>
<li>A and The in similar space</li>
<li>“A dog was running in a room” - never encountered in training set –&gt; u are out of distribution</li>
<li>Probabilistic approach allows u to get around it –&gt; May have seem similar sentences in training , “A” and “The” may seem to be frequently interchangable and therefore, may occupy similar space [“Cats” and “Dogs” are animal and they cooccur in very similar context]</li>
<li>Embedding helps in overcoming limitations and transfer knowledge through that way which helps in generalizing better</li>
<li><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-3-1-image-2.png" class="img-fluid figure-img"></p>
<figcaption>image-2.png</figcaption>
</figure>
</div>
<ul>
<li>Matrix C is shared across the parameters/ words</li>
<li>Size of hidden layer is hyperparameter - this can be as large or as small. We will vary sizes and evalate how they work.</li>
<li>Output layer is with the softmax layer. –&gt; 17K neurons- 17K logits -&gt; softmax fully connected to all parameter in hidden layersoutput parameters</li>
<li>Parameters
<ul>
<li>Weights and biases of output layer</li>
<li>Weights and biases of hidden layer</li>
<li>Look up table C [ How is this lookup table implemented??]</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="read-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="read-the-dataset">Read the dataset</h2>
<div id="cell-5" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">open</span>(<span class="st">"../data/names.txt"</span>, <span class="st">'r'</span>).read().split()<span class="op">;</span> words[:<span class="dv">5</span>]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>words[:<span class="dv">8</span>], <span class="bu">len</span>(words)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia'],
 32033)</code></pre>
</div>
</div>
<div id="cell-6" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>s2i <span class="op">=</span> stoi(words, start_str<span class="op">=</span><span class="st">"."</span>, end_str<span class="op">=</span><span class="st">"."</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>i2s <span class="op">=</span> itos(s2i)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>display(s2i)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>display(i2s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>{'a': 1,
 'b': 2,
 'c': 3,
 'd': 4,
 'e': 5,
 'f': 6,
 'g': 7,
 'h': 8,
 'i': 9,
 'j': 10,
 'k': 11,
 'l': 12,
 'm': 13,
 'n': 14,
 'o': 15,
 'p': 16,
 'q': 17,
 'r': 18,
 's': 19,
 't': 20,
 'u': 21,
 'v': 22,
 'w': 23,
 'x': 24,
 'y': 25,
 'z': 26,
 '.': 0}</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{1: 'a',
 2: 'b',
 3: 'c',
 4: 'd',
 5: 'e',
 6: 'f',
 7: 'g',
 8: 'h',
 9: 'i',
 10: 'j',
 11: 'k',
 12: 'l',
 13: 'm',
 14: 'n',
 15: 'o',
 16: 'p',
 17: 'q',
 18: 'r',
 19: 's',
 20: 't',
 21: 'u',
 22: 'v',
 23: 'w',
 24: 'x',
 25: 'y',
 26: 'z',
 0: '.'}</code></pre>
</div>
</div>
<hr>
<p><a href="https://github.com/Rahuketu86/minion/blob/main/minion/makemore/mlp.py#L18" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="build_xy" class="level3">
<h3 class="anchored" data-anchor-id="build_xy">build_XY</h3>
<blockquote class="blockquote">
<pre><code> build_XY (words, s2i, block_size, str_term='.', verbose=False)</code></pre>
</blockquote>
<div id="cell-8" class="cell">
<details open="" class="code-fold">
<summary>Exported source</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_XY(words, s2i, block_size, str_term<span class="op">=</span><span class="st">"."</span>, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> [s2i[str_term]]<span class="op">*</span>block_size  <span class="co"># We need numerical embedding for each character 0 is for "."</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> str_term:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> verbose: </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>                i2s <span class="op">=</span> itos(s2i)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">""</span>.join([i2s[i] <span class="cf">for</span> i <span class="kw">in</span> context]),<span class="st">"--&gt;"</span>, ch)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>            X.append(context)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>            Y.append(s2i[ch])</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [s2i[ch]]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor(X), torch.tensor(Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-9" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> build_XY(words[:<span class="dv">2</span>], s2i, block_size<span class="op">=</span><span class="dv">5</span>, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X, Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>..... --&gt; e
....e --&gt; m
...em --&gt; m
..emm --&gt; a
.emma --&gt; .
..... --&gt; o
....o --&gt; l
...ol --&gt; i
..oli --&gt; v
.oliv --&gt; i
olivi --&gt; a
livia --&gt; .</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[ 0,  0,  0,  0,  0],
         [ 0,  0,  0,  0,  5],
         [ 0,  0,  0,  5, 13],
         [ 0,  0,  5, 13, 13],
         [ 0,  5, 13, 13,  1],
         [ 0,  0,  0,  0,  0],
         [ 0,  0,  0,  0, 15],
         [ 0,  0,  0, 15, 12],
         [ 0,  0, 15, 12,  9],
         [ 0, 15, 12,  9, 22],
         [15, 12,  9, 22,  9],
         [12,  9, 22,  9,  1]]),
 tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0]))</code></pre>
</div>
</div>
<div id="cell-10" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> build_XY(words[:<span class="dv">2</span>], s2i, block_size<span class="op">=</span><span class="dv">3</span>, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>X, Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>... --&gt; e
..e --&gt; m
.em --&gt; m
emm --&gt; a
mma --&gt; .
... --&gt; o
..o --&gt; l
.ol --&gt; i
oli --&gt; v
liv --&gt; i
ivi --&gt; a
via --&gt; .</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[ 0,  0,  0],
         [ 0,  0,  5],
         [ 0,  5, 13],
         [ 5, 13, 13],
         [13, 13,  1],
         [ 0,  0,  0],
         [ 0,  0, 15],
         [ 0, 15, 12],
         [15, 12,  9],
         [12,  9, 22],
         [ 9, 22,  9],
         [22,  9,  1]]),
 tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0]))</code></pre>
</div>
</div>
<div id="cell-11" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> build_XY(words, s2i, block_size<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>X.shape, Y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([228146, 3]), torch.Size([228146]))</code></pre>
</div>
</div>
<div id="cell-12" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> build_XY(words[:<span class="dv">5</span>], s2i, block_size<span class="op">=</span><span class="dv">3</span>, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>X.shape, X.dtype, Y.shape, Y.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>... --&gt; e
..e --&gt; m
.em --&gt; m
emm --&gt; a
mma --&gt; .
... --&gt; o
..o --&gt; l
.ol --&gt; i
oli --&gt; v
liv --&gt; i
ivi --&gt; a
via --&gt; .
... --&gt; a
..a --&gt; v
.av --&gt; a
ava --&gt; .
... --&gt; i
..i --&gt; s
.is --&gt; a
isa --&gt; b
sab --&gt; e
abe --&gt; l
bel --&gt; l
ell --&gt; a
lla --&gt; .
... --&gt; s
..s --&gt; o
.so --&gt; p
sop --&gt; h
oph --&gt; i
phi --&gt; a
hia --&gt; .</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)</code></pre>
</div>
</div>
</section>
</section>
<section id="implementing-neural-network-step-by-step" class="level2">
<h2 class="anchored" data-anchor-id="implementing-neural-network-step-by-step">Implementing Neural Network Step by Step</h2>
<section id="building-embedding-lookup-table-c" class="level3">
<h3 class="anchored" data-anchor-id="building-embedding-lookup-table-c">Building embedding lookup table C</h3>
<ul>
<li>27 character and we need to embed them in lower dimensional space.</li>
<li>In paper they crammed 17K words in lower dimensional space (30)</li>
<li>27 characters with 2 dimensional space</li>
</ul>
<div id="cell-16" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.randn((<span class="bu">len</span>(s2i),<span class="dv">2</span>), generator<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">2147483647</span>))<span class="op">;</span> C.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([27, 2])</code></pre>
</div>
</div>
<div id="cell-17" class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>C[<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.4713,  0.7868])</code></pre>
</div>
</div>
<div id="cell-18" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> F.one_hot(torch.tensor(<span class="dv">5</span>), num_classes<span class="op">=</span><span class="bu">len</span>(s2i)) </span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>a.<span class="bu">float</span>()<span class="op">@</span>C</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.4713,  0.7868])</code></pre>
</div>
</div>
<ul>
<li>If we take one_hot vector and multiply by <code>C</code></li>
<li>Embedding of integer can be thought of as
<ul>
<li>Indexing integer in a look up table <code>C</code></li>
<li>Also we can think of this piece as first layer of NN- No activation-Weight matrix in C. Simply a lookup</li>
</ul></li>
</ul>
<p>Now looking up a value for a character in C is simply looking up an index.</p>
<div id="cell-20" class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>C[<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.4713,  0.7868])</code></pre>
</div>
</div>
<p>We can also do one-hot encoding of our charactar array and then provide one - hot vector and multiply by C which will pluck / lookup right weight for corresponding character</p>
<div id="cell-22" class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>c5 <span class="op">=</span> F.one_hot(torch.tensor(<span class="dv">5</span>), num_classes<span class="op">=</span><span class="bu">len</span>(s2i)).<span class="bu">float</span>() <span class="co"># One hot vector for 5th character + put input as tensor of float not int or tensor[int]</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>c5<span class="op">@</span>C</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.4713,  0.7868])</code></pre>
</div>
</div>
<p>Above implies we can think of embedding in 2 ways</p>
<ul>
<li>Integer indexing into a look up table <code>C</code></li>
<li>Or First layer of bigger neural network
<ul>
<li>This embedding layer have neuron that have no non-linearity/ activation - linear</li>
<li>And their weight matrix is <code>C</code></li>
<li>We are just encoding the character to one-hot and feeding into NN and first layers basically embeds them</li>
<li>We index them because it’s much much faster</li>
</ul></li>
</ul>
</section>
<section id="sidenote-on-pytorch-indexing" class="level3">
<h3 class="anchored" data-anchor-id="sidenote-on-pytorch-indexing">Sidenote on Pytorch indexing</h3>
<section id="index-on-int" class="level4">
<h4 class="anchored" data-anchor-id="index-on-int">Index on int</h4>
<div id="cell-26" class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>C[<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.4713,  0.7868])</code></pre>
</div>
</div>
</section>
<section id="index-on-list" class="level4">
<h4 class="anchored" data-anchor-id="index-on-list">Index on list</h4>
<div id="cell-28" class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>C[[<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-0.4713,  0.7868],
        [-0.3284, -0.4330],
        [ 1.3729,  2.9334]])</code></pre>
</div>
</div>
</section>
<section id="index-on-tensors" class="level4">
<h4 class="anchored" data-anchor-id="index-on-tensors">Index on tensors</h4>
<div id="cell-30" class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>C[torch.tensor([<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>])]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-0.4713,  0.7868],
        [-0.3284, -0.4330],
        [ 1.3729,  2.9334]])</code></pre>
</div>
</div>
<div id="cell-31" class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>C[torch.tensor([<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>, <span class="dv">7</span>])]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-0.4713,  0.7868],
        [-0.3284, -0.4330],
        [ 1.3729,  2.9334],
        [ 1.3729,  2.9334]])</code></pre>
</div>
</div>
</section>
<section id="index-with-multidimensional-tensor-of-integers" class="level4">
<h4 class="anchored" data-anchor-id="index-with-multidimensional-tensor-of-integers">Index with multidimensional tensor of integers</h4>
<div id="cell-33" class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>display(C[X].shape)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>display(C[X][:<span class="dv">5</span>])</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>display(C[X][:<span class="dv">5</span>].shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([32, 3, 2])</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-0.4713,  0.7868]],

        [[ 1.5674, -0.2373],
         [-0.4713,  0.7868],
         [ 2.4448, -0.6701]],

        [[-0.4713,  0.7868],
         [ 2.4448, -0.6701],
         [ 2.4448, -0.6701]],

        [[ 2.4448, -0.6701],
         [ 2.4448, -0.6701],
         [-0.0274, -1.1008]]])</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([5, 3, 2])</code></pre>
</div>
</div>
</section>
</section>
<section id="implementing-embedding-lookup-table" class="level3">
<h3 class="anchored" data-anchor-id="implementing-embedding-lookup-table">Implementing embedding lookup table</h3>
<div id="cell-35" class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>C[X]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-0.4713,  0.7868]],

        [[ 1.5674, -0.2373],
         [-0.4713,  0.7868],
         [ 2.4448, -0.6701]],

        [[-0.4713,  0.7868],
         [ 2.4448, -0.6701],
         [ 2.4448, -0.6701]],

        [[ 2.4448, -0.6701],
         [ 2.4448, -0.6701],
         [-0.0274, -1.1008]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-1.0725,  0.7276]],

        [[ 1.5674, -0.2373],
         [-1.0725,  0.7276],
         [-0.0707,  2.4968]],

        [[-1.0725,  0.7276],
         [-0.0707,  2.4968],
         [ 0.6772, -0.8404]],

        [[-0.0707,  2.4968],
         [ 0.6772, -0.8404],
         [-0.1158, -1.2078]],

        [[ 0.6772, -0.8404],
         [-0.1158, -1.2078],
         [ 0.6772, -0.8404]],

        [[-0.1158, -1.2078],
         [ 0.6772, -0.8404],
         [-0.0274, -1.1008]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-0.0274, -1.1008]],

        [[ 1.5674, -0.2373],
         [-0.0274, -1.1008],
         [-0.1158, -1.2078]],

        [[-0.0274, -1.1008],
         [-0.1158, -1.2078],
         [-0.0274, -1.1008]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 0.6772, -0.8404]],

        [[ 1.5674, -0.2373],
         [ 0.6772, -0.8404],
         [ 0.1476, -1.0006]],

        [[ 0.6772, -0.8404],
         [ 0.1476, -1.0006],
         [-0.0274, -1.1008]],

        [[ 0.1476, -1.0006],
         [-0.0274, -1.1008],
         [ 0.2859, -0.0296]],

        [[-0.0274, -1.1008],
         [ 0.2859, -0.0296],
         [-0.4713,  0.7868]],

        [[ 0.2859, -0.0296],
         [-0.4713,  0.7868],
         [-0.0707,  2.4968]],

        [[-0.4713,  0.7868],
         [-0.0707,  2.4968],
         [-0.0707,  2.4968]],

        [[-0.0707,  2.4968],
         [-0.0707,  2.4968],
         [-0.0274, -1.1008]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 0.1476, -1.0006]],

        [[ 1.5674, -0.2373],
         [ 0.1476, -1.0006],
         [-1.0725,  0.7276]],

        [[ 0.1476, -1.0006],
         [-1.0725,  0.7276],
         [ 0.0511,  1.3095]],

        [[-1.0725,  0.7276],
         [ 0.0511,  1.3095],
         [ 1.5618, -1.6261]],

        [[ 0.0511,  1.3095],
         [ 1.5618, -1.6261],
         [ 0.6772, -0.8404]],

        [[ 1.5618, -1.6261],
         [ 0.6772, -0.8404],
         [-0.0274, -1.1008]]])</code></pre>
</div>
</div>
<div id="cell-36" class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>X.shape, C[X].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([32, 3]), torch.Size([32, 3, 2]))</code></pre>
</div>
</div>
<p>We have a character input of 32 by 3 . For each of them we have gotten an embedding vector</p>
<div id="cell-38" class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>display(X[<span class="dv">13</span>,<span class="dv">2</span>])</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>C[X][<span class="dv">13</span>, <span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(1)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.0274, -1.1008])</code></pre>
</div>
</div>
<div id="cell-39" class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>C[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.0274, -1.1008])</code></pre>
</div>
</div>
<div id="cell-40" class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> C[X]</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>emb.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([32, 3, 2])</code></pre>
</div>
</div>
</section>
<section id="implementing-the-hidden-layer-internals-of-torch.tensor-storage-views" class="level3">
<h3 class="anchored" data-anchor-id="implementing-the-hidden-layer-internals-of-torch.tensor-storage-views">Implementing the hidden layer + internals of torch.Tensor: storage, views</h3>
<ul>
<li>We are going to stack 3 characters . So number of features for hidden layers are 6 (3*2)</li>
<li>We can choose any number of neurons for hidden layer. Let’s start by choosing 100 neurons</li>
<li>Size of hidden layer (features, neurons)</li>
</ul>
<div id="cell-43" class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> torch.randn((<span class="dv">6</span>, <span class="dv">100</span>))</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> torch.randn(<span class="dv">100</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>W1.shape, b1.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([6, 100]), torch.Size([100]))</code></pre>
</div>
</div>
<p>What we want to do emb@W1+b1?</p>
<ul>
<li>Above won’t work</li>
<li>emb size is 32<em>3</em>2</li>
<li>W1 size is 6*100</li>
<li>We somehow need to concatenate multiple embeddings into single tensor array</li>
</ul>
<div id="cell-45" class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>torch.cat?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Docstring:
cat(tensors, dim=0, *, out=None) -&gt; Tensor

Concatenates the given sequence of :attr:`seq` tensors in the given dimension.
All tensors must either have the same shape (except in the concatenating
dimension) or be empty.

:func:`torch.cat` can be seen as an inverse operation for :func:`torch.split`
and :func:`torch.chunk`.

:func:`torch.cat` can be best understood via examples.

Args:
    tensors (sequence of Tensors): any python sequence of tensors of the same type.
        Non-empty tensors provided must have the same shape, except in the
        cat dimension.
    dim (int, optional): the dimension over which the tensors are concatenated

Keyword args:
    out (Tensor, optional): the output tensor.

Example::

    &gt;&gt;&gt; x = torch.randn(2, 3)
    &gt;&gt;&gt; x
    tensor([[ 0.6580, -1.0969, -0.4614],
            [-0.1034, -0.5790,  0.1497]])
    &gt;&gt;&gt; torch.cat((x, x, x), 0)
    tensor([[ 0.6580, -1.0969, -0.4614],
            [-0.1034, -0.5790,  0.1497],
            [ 0.6580, -1.0969, -0.4614],
            [-0.1034, -0.5790,  0.1497],
            [ 0.6580, -1.0969, -0.4614],
            [-0.1034, -0.5790,  0.1497]])
    &gt;&gt;&gt; torch.cat((x, x, x), 1)
    tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
             -1.0969, -0.4614],
            [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
             -0.5790,  0.1497]])
Type:      builtin_function_or_method</code></pre>
</div>
</div>
<div id="cell-46" class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>emb.shape</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>emb[:, <span class="dv">0</span>, :].shape, emb[:, <span class="dv">1</span>, :].shape, emb[:, <span class="dv">2</span>, :].shape</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>torch.cat([emb[:, <span class="dv">0</span>, :], emb[:, <span class="dv">1</span>, :], emb[:, <span class="dv">2</span>, :]], dim<span class="op">=</span><span class="dv">1</span>).shape <span class="co"># Not scalable with larger context</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([32, 6])</code></pre>
</div>
</div>
<div id="cell-47" class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.unbind(emb, <span class="dv">1</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(a)<span class="op">;</span> a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-0.4713,  0.7868],
         [ 2.4448, -0.6701],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-1.0725,  0.7276],
         [-0.0707,  2.4968],
         [ 0.6772, -0.8404],
         [-0.1158, -1.2078],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-0.0274, -1.1008],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 0.6772, -0.8404],
         [ 0.1476, -1.0006],
         [-0.0274, -1.1008],
         [ 0.2859, -0.0296],
         [-0.4713,  0.7868],
         [-0.0707,  2.4968],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 0.1476, -1.0006],
         [-1.0725,  0.7276],
         [ 0.0511,  1.3095],
         [ 1.5618, -1.6261]]),
 tensor([[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-0.4713,  0.7868],
         [ 2.4448, -0.6701],
         [ 2.4448, -0.6701],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-1.0725,  0.7276],
         [-0.0707,  2.4968],
         [ 0.6772, -0.8404],
         [-0.1158, -1.2078],
         [ 0.6772, -0.8404],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-0.0274, -1.1008],
         [-0.1158, -1.2078],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 0.6772, -0.8404],
         [ 0.1476, -1.0006],
         [-0.0274, -1.1008],
         [ 0.2859, -0.0296],
         [-0.4713,  0.7868],
         [-0.0707,  2.4968],
         [-0.0707,  2.4968],
         [ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 0.1476, -1.0006],
         [-1.0725,  0.7276],
         [ 0.0511,  1.3095],
         [ 1.5618, -1.6261],
         [ 0.6772, -0.8404]]),
 tensor([[ 1.5674, -0.2373],
         [-0.4713,  0.7868],
         [ 2.4448, -0.6701],
         [ 2.4448, -0.6701],
         [-0.0274, -1.1008],
         [ 1.5674, -0.2373],
         [-1.0725,  0.7276],
         [-0.0707,  2.4968],
         [ 0.6772, -0.8404],
         [-0.1158, -1.2078],
         [ 0.6772, -0.8404],
         [-0.0274, -1.1008],
         [ 1.5674, -0.2373],
         [-0.0274, -1.1008],
         [-0.1158, -1.2078],
         [-0.0274, -1.1008],
         [ 1.5674, -0.2373],
         [ 0.6772, -0.8404],
         [ 0.1476, -1.0006],
         [-0.0274, -1.1008],
         [ 0.2859, -0.0296],
         [-0.4713,  0.7868],
         [-0.0707,  2.4968],
         [-0.0707,  2.4968],
         [-0.0274, -1.1008],
         [ 1.5674, -0.2373],
         [ 0.1476, -1.0006],
         [-1.0725,  0.7276],
         [ 0.0511,  1.3095],
         [ 1.5618, -1.6261],
         [ 0.6772, -0.8404],
         [-0.0274, -1.1008]]))</code></pre>
</div>
</div>
<div id="cell-48" class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>torch.cat(torch.unbind(emb, <span class="dv">1</span>), <span class="dv">1</span>).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([32, 6])</code></pre>
</div>
</div>
<div id="cell-49" class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.arange(<span class="dv">18</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>a.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([18])</code></pre>
</div>
</div>
<div id="cell-50" class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>a.view(<span class="dv">2</span>, <span class="dv">9</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>a.view(<span class="dv">9</span>,<span class="dv">2</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>a.view(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>) <span class="co"># Extremely efficient</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[[ 0,  1],
         [ 2,  3],
         [ 4,  5]],

        [[ 6,  7],
         [ 8,  9],
         [10, 11]],

        [[12, 13],
         [14, 15],
         [16, 17]]])</code></pre>
</div>
</div>
<div id="cell-51" class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>a.storage()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code> 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
 13
 14
 15
 16
 17
[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>In pytorch, tensor is always stored as a 1 dimensional vector ( represented in computer memory)</li>
<li>When calling view - internals of tensors - storage offsets, strides and shapes are being manipulated</li>
<li>Calling view is extremely efficient . No memory is being changed</li>
<li>Concatenation is much less efficient - New memory is being created when u use concatenation(Whole new tensor with a whole new storage). No way of concatenating the tensor by manipulating the view attribute</li>
<li>Tensor shape is just a logical view of physical memory <img src="04_makemore.mlp_files/figure-html/cell-52-1-image.png" class="img-fluid" alt="image.png"></li>
<li>More on Pytorch internals is <a href="http://blog.ezyang.com/2019/05/pytorch-internals/">here</a></li>
</ul>
</div>
</div>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>emb.shape</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>emb[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 1.5674, -0.2373],
        [ 1.5674, -0.2373],
        [-0.4713,  0.7868]])</code></pre>
</div>
</div>
<div id="cell-54" class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>emb.view(emb.shape[<span class="dv">0</span>],emb.shape[<span class="dv">1</span>]<span class="op">*</span>emb.shape[<span class="dv">2</span>])[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([ 1.5674, -0.2373,  1.5674, -0.2373, -0.4713,  0.7868])</code></pre>
</div>
</div>
<div id="cell-55" class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>emb.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>)[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([ 1.5674, -0.2373,  1.5674, -0.2373, -0.4713,  0.7868])</code></pre>
</div>
</div>
<div id="cell-56" class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([ 1.5674, -0.2373,  1.5674, -0.2373, -0.4713,  0.7868])</code></pre>
</div>
</div>
<div id="cell-57" class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, emb.shape[<span class="dv">1</span>]<span class="op">*</span>emb.shape[<span class="dv">2</span>])<span class="op">@</span>W1 <span class="op">+</span> b1)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 0.9696, -0.0761,  0.0969,  ...,  0.8818, -0.9987,  0.9760],
        [-0.9849, -0.7707, -0.9131,  ...,  0.9966, -0.9847,  0.9979],
        [ 0.9986,  0.9569, -0.3098,  ..., -0.9844, -0.5528, -0.0351],
        ...,
        [ 0.9896,  0.9411, -0.8544,  ...,  0.9877, -0.9991, -0.9401],
        [ 1.0000, -0.4195,  0.4700,  ...,  0.9998, -0.9997,  0.9730],
        [-0.1685,  0.9893,  0.9984,  ...,  0.5667, -0.9995,  0.9994]])</code></pre>
</div>
</div>
</section>
<section id="final-layer" class="level3">
<h3 class="anchored" data-anchor-id="final-layer">Final layer</h3>
<div id="cell-59" class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> torch.randn((<span class="dv">100</span>, <span class="bu">len</span>(s2i)))</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> torch.randn(<span class="bu">len</span>(s2i))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-60" class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>W2.shape, W2.nelement()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([100, 27]), 2700)</code></pre>
</div>
</div>
<div id="cell-61" class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> h<span class="op">@</span>W2<span class="op">+</span>b2<span class="op">;</span> logits.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([32, 27])</code></pre>
</div>
</div>
<div id="cell-62" class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-63" class="cell">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> prob.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([32, 27])</code></pre>
</div>
</div>
<div id="cell-64" class="cell">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>prob.<span class="bu">sum</span>(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])</code></pre>
</div>
</div>
</section>
</section>
<section id="refactoring" class="level2">
<h2 class="anchored" data-anchor-id="refactoring">Refactoring</h2>
<section id="model-class" class="level3">
<h3 class="anchored" data-anchor-id="model-class">Model Class</h3>
<hr>
<p><a href="https://github.com/Rahuketu86/minion/blob/main/minion/makemore/mlp.py#L32" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="model" class="level3">
<h3 class="anchored" data-anchor-id="model">Model</h3>
<blockquote class="blockquote">
<pre><code> Model (vocab_sz, blck_sz=3, emb_sz=2, hidden_units=100,
        g=&lt;torch._C.Generator object at 0x7f7709087910&gt;)</code></pre>
</blockquote>
<p><em>Initialize self. See help(type(self)) for accurate signature.</em></p>
<div id="cell-68" class="cell">
<details open="" class="code-fold">
<summary>Exported source</summary>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(<span class="bu">object</span>):</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, blck_sz<span class="op">=</span><span class="dv">3</span>, emb_sz<span class="op">=</span><span class="dv">2</span>, hidden_units<span class="op">=</span><span class="dv">100</span>, g<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">2147483647</span>)) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C <span class="op">=</span> torch.randn((vocab_sz,emb_sz), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> torch.randn((blck_sz<span class="op">*</span>emb_sz, hidden_units), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">=</span> torch.randn(hidden_units, generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> torch.randn((hidden_units, vocab_sz), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">=</span> torch.randn(vocab_sz, generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blck_sz <span class="op">=</span> blck_sz</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.emb_sz <span class="op">=</span> emb_sz</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_units <span class="op">=</span> hidden_units</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, X):</span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>        emb <span class="op">=</span> <span class="va">self</span>.C[X]</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> torch.tanh(emb.view(<span class="op">-</span><span class="dv">1</span>, emb.shape[<span class="dv">1</span>]<span class="op">*</span>emb.shape[<span class="dv">2</span>])<span class="op">@</span>self.W1 <span class="op">+</span> <span class="va">self</span>.b1)</span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> h<span class="op">@</span>self.W2 <span class="op">+</span> <span class="va">self</span>.b2</span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.C, <span class="va">self</span>.W1, <span class="va">self</span>.b1, <span class="va">self</span>.W2, <span class="va">self</span>.b2]</span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> num_params(<span class="va">self</span>):</span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters())</span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb89-24"><a href="#cb89-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>):</span>
<span id="cb89-25"><a href="#cb89-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters(): </span>
<span id="cb89-26"><a href="#cb89-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print(p.shape, p.data, p.grad)</span></span>
<span id="cb89-27"><a href="#cb89-27" aria-hidden="true" tabindex="-1"></a>            p.grad <span class="op">=</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-69" class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>F.cross_entropy??</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Signature:
F.cross_entropy(
    input: torch.Tensor,
    target: torch.Tensor,
    weight: Optional[torch.Tensor] = None,
    size_average: Optional[bool] = None,
    ignore_index: int = -100,
    reduce: Optional[bool] = None,
    reduction: str = 'mean',
    label_smoothing: float = 0.0,
) -&gt; torch.Tensor
Source:   
def cross_entropy(
    input: Tensor,
    target: Tensor,
    weight: Optional[Tensor] = None,
    size_average: Optional[bool] = None,
    ignore_index: int = -100,
    reduce: Optional[bool] = None,
    reduction: str = "mean",
    label_smoothing: float = 0.0,
) -&gt; Tensor:
    r"""This criterion computes the cross entropy loss between input logits and target.

    See :class:`~torch.nn.CrossEntropyLoss` for details.

    Args:
        input (Tensor) : Predicted unnormalized logits;
            see Shape section below for supported shapes.
        target (Tensor) : Ground truth class indices or class probabilities;
            see Shape section below for supported shapes.
        weight (Tensor, optional): a manual rescaling weight given to each
            class. If given, has to be a Tensor of size `C`
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        ignore_index (int, optional): Specifies a target value that is ignored
            and does not contribute to the input gradient. When :attr:`size_average` is
            ``True``, the loss is averaged over non-ignored targets. Note that
            :attr:`ignore_index` is only applicable when the target contains class indices.
            Default: -100
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (str, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
        label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount
            of smoothing when computing the loss, where 0.0 means no smoothing. The targets
            become a mixture of the original ground truth and a uniform distribution as described in
            `Rethinking the Inception Architecture for Computer Vision &lt;https://arxiv.org/abs/1512.00567&gt;`__. Default: :math:`0.0`.

    Shape:
        - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`
          in the case of `K`-dimensional loss.
        - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with
          :math:`K \geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.
          If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.

        where:

        .. math::
            \begin{aligned}
                C ={} &amp; \text{number of classes} \\
                N ={} &amp; \text{batch size} \\
            \end{aligned}

    Examples::

        &gt;&gt;&gt; # Example of target with class indices
        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
        &gt;&gt;&gt; target = torch.randint(5, (3,), dtype=torch.int64)
        &gt;&gt;&gt; loss = F.cross_entropy(input, target)
        &gt;&gt;&gt; loss.backward()
        &gt;&gt;&gt;
        &gt;&gt;&gt; # Example of target with class probabilities
        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
        &gt;&gt;&gt; target = torch.randn(3, 5).softmax(dim=1)
        &gt;&gt;&gt; loss = F.cross_entropy(input, target)
        &gt;&gt;&gt; loss.backward()
    """
    if has_torch_function_variadic(input, target, weight):
        return handle_torch_function(
            cross_entropy,
            (input, target, weight),
            input,
            target,
            weight=weight,
            size_average=size_average,
            ignore_index=ignore_index,
            reduce=reduce,
            reduction=reduction,
            label_smoothing=label_smoothing,
        )
    if size_average is not None or reduce is not None:
        reduction = _Reduction.legacy_get_string(size_average, reduce)
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
File:      /opt/homebrew/Caskroom/miniforge/base/envs/aiking/lib/python3.9/site-packages/torch/nn/functional.py
Type:      function</code></pre>
</div>
</div>
</section>
<section id="model-loss" class="level3">
<h3 class="anchored" data-anchor-id="model-loss">Model loss</h3>
<hr>
<p><a href="https://github.com/Rahuketu86/minion/blob/main/minion/makemore/mlp.py#L62" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="softmax" class="level3">
<h3 class="anchored" data-anchor-id="softmax">softmax</h3>
<blockquote class="blockquote">
<pre><code> softmax (inputs, dim=1)</code></pre>
</blockquote>
<div id="cell-72" class="cell">
<details open="" class="code-fold">
<summary>Exported source</summary>
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(inputs, dim<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>    c  <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">max</span>(inputs)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> (inputs<span class="op">+</span>c).exp()</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> counts<span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span>dim, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> probs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-73" class="cell">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]]).<span class="bu">float</span>()</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>softmax(inputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0.0900, 0.2447, 0.6652]])</code></pre>
</div>
</div>
<div id="cell-74" class="cell">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>inputs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 3])</code></pre>
</div>
</div>
<div id="cell-75" class="cell">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>F.softmax(inputs, dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0.0900, 0.2447, 0.6652]])</code></pre>
</div>
</div>
<hr>
<p><a href="https://github.com/Rahuketu86/minion/blob/main/minion/makemore/mlp.py#L69" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="nll" class="level3">
<h3 class="anchored" data-anchor-id="nll">nll</h3>
<blockquote class="blockquote">
<pre><code> nll (inputs, target)</code></pre>
</blockquote>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>inputs</td>
<td>Takes logits</td>
</tr>
<tr class="even">
<td>target</td>
<td>Takes y</td>
</tr>
</tbody>
</table>
<div id="cell-77" class="cell">
<details open="" class="code-fold">
<summary>Exported source</summary>
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nll(inputs,  <span class="co">#Takes logits</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>        target  <span class="co">#Takes y</span></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>        ): </span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># counts = inputs.exp()</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># c  = -torch.max(inputs)</span></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># counts = (inputs+c).exp()</span></span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># probs = counts/ counts.sum(dim=1, keepdim=True)</span></span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> softmax(inputs, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>probs[torch.arange(<span class="bu">len</span>(target)), target].log().mean()</span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="forward-pass-and-update" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass-and-update">Forward Pass and Update</h3>
<div id="cell-79" class="cell">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> build_XY(words[:<span class="dv">5</span>], s2i, block_size<span class="op">=</span><span class="dv">3</span>)<span class="op">;</span> X.shape, Y.shape</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i))</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward Pass</span></span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> model(X)<span class="op">;</span> logits.shape </span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = nll(logits, Y)</span></span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> F.cross_entropy(logits, Y)</span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>loss2 <span class="op">=</span> nll(logits, Y)</span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Backward Pass</span></span>
<span id="cb102-12"><a href="#cb102-12" aria-hidden="true" tabindex="-1"></a>model.zero_grad()</span>
<span id="cb102-13"><a href="#cb102-13" aria-hidden="true" tabindex="-1"></a>loss.backward()<span class="op">;</span> loss, loss2</span>
<span id="cb102-14"><a href="#cb102-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-15"><a href="#cb102-15" aria-hidden="true" tabindex="-1"></a><span class="co"># # Update</span></span>
<span id="cb102-16"><a href="#cb102-16" aria-hidden="true" tabindex="-1"></a><span class="co"># for p in model.parameters():</span></span>
<span id="cb102-17"><a href="#cb102-17" aria-hidden="true" tabindex="-1"></a><span class="co">#     p.data -= lr*p.grad</span></span>
<span id="cb102-18"><a href="#cb102-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-19"><a href="#cb102-19" aria-hidden="true" tabindex="-1"></a><span class="co"># model.parameters()[0].grad</span></span>
<span id="cb102-20"><a href="#cb102-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-21"><a href="#cb102-21" aria-hidden="true" tabindex="-1"></a><span class="co"># model.parameters()[1].grad</span></span>
<span id="cb102-22"><a href="#cb102-22" aria-hidden="true" tabindex="-1"></a><span class="co"># len(model.parameters())</span></span>
<span id="cb102-23"><a href="#cb102-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-24"><a href="#cb102-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb102-25"><a href="#cb102-25" aria-hidden="true" tabindex="-1"></a>    p.data <span class="op">-=</span>lr<span class="op">*</span>p.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="minibatch-construct" class="level3">
<h3 class="anchored" data-anchor-id="minibatch-construct">Minibatch Construct</h3>
<div id="cell-81" class="cell">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.randint()</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>display(X.shape)</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span>(<span class="dv">3</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([32, 3])</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>tensor([0, 6, 8])</code></pre>
</div>
</div>
<div id="cell-82" class="cell">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>ix <span class="op">=</span> torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span>X.shape[<span class="dv">0</span>], size<span class="op">=</span>(batch_size,))</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>X[ix].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([32, 3])</code></pre>
</div>
</div>
<div id="cell-83" class="cell">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> model.C.detach().numpy()<span class="op">;</span> a.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(27, 2)</code></pre>
</div>
</div>
<div id="cell-84" class="cell">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(a[:,<span class="dv">0</span>], a[:,<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-59-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
<p><a href="https://github.com/Rahuketu86/minion/blob/main/minion/makemore/mlp.py#L82" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="plot_embeddings" class="level3">
<h3 class="anchored" data-anchor-id="plot_embeddings">plot_embeddings</h3>
<blockquote class="blockquote">
<pre><code> plot_embeddings (model, s2i, emb_model=None, cluster_model=None)</code></pre>
</blockquote>
<div id="cell-86" class="cell" data-execution_count="619">
<details open="" class="code-fold">
<summary>Exported source</summary>
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_embeddings(model, s2i, emb_model<span class="op">=</span><span class="va">None</span>, cluster_model <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>    i2s <span class="op">=</span> itos(s2i)</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> model.C.detach().numpy()</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> emb_model: a <span class="op">=</span> c</span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: a <span class="op">=</span> emb_model.fit_transform(c)</span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> cluster_model: plt.scatter(a[:,<span class="dv">0</span>], a[:,<span class="dv">1</span>], s<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> cluster_model.fit_predict(c)</span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a>        plt.scatter(a[:,<span class="dv">0</span>], a[:,<span class="dv">1</span>], s<span class="op">=</span><span class="dv">200</span>, c<span class="op">=</span>label)</span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(a.shape[<span class="dv">0</span>]):</span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plt.text()</span></span>
<span id="cb112-14"><a href="#cb112-14" aria-hidden="true" tabindex="-1"></a>        plt.text(a[i,<span class="dv">0</span>], a[i,<span class="dv">1</span>], i2s[i], ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb112-15"><a href="#cb112-15" aria-hidden="true" tabindex="-1"></a>    plt.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-87" class="cell" data-execution_count="620">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i))</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-62-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-88" class="cell" data-execution_count="621">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i))</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i, cluster_model<span class="op">=</span>KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/homebrew/Caskroom/miniforge/base/envs/aiking/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-63-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="training-function" class="level3">
<h3 class="anchored" data-anchor-id="training-function">Training Function</h3>
<hr>
<p><a href="https://github.com/Rahuketu86/minion/blob/main/minion/makemore/mlp.py#L99" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="train" class="level3">
<h3 class="anchored" data-anchor-id="train">train</h3>
<blockquote class="blockquote">
<pre><code> train (model, X, Y, lr=0.1, epochs=1000, verbose=False, batch_sz=None,
        loss_fn=&lt;function cross_entropy&gt;, tracker=None)</code></pre>
</blockquote>
<div id="cell-91" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Exported source</summary>
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, X, Y, lr<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">1000</span>, verbose<span class="op">=</span><span class="va">False</span>, batch_sz<span class="op">=</span><span class="va">None</span>, loss_fn<span class="op">=</span>F.cross_entropy, tracker <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">#minibatch construct</span></span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> X</span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> Y</span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch_sz:</span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a>            ix <span class="op">=</span> torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span>X.shape[<span class="dv">0</span>], size<span class="op">=</span>(batch_sz,))</span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">=</span> X[ix]</span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a>            target <span class="op">=</span> Y[ix]</span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward Pass</span></span>
<span id="cb117-15"><a href="#cb117-15" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(inputs)<span class="op">;</span> logits.shape </span>
<span id="cb117-16"><a href="#cb117-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss = nll(logits, Y)</span></span>
<span id="cb117-17"><a href="#cb117-17" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(logits, target)</span>
<span id="cb117-18"><a href="#cb117-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss2 = nll(logits, Y)</span></span>
<span id="cb117-19"><a href="#cb117-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-20"><a href="#cb117-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward Pass</span></span>
<span id="cb117-21"><a href="#cb117-21" aria-hidden="true" tabindex="-1"></a>        model.zero_grad()</span>
<span id="cb117-22"><a href="#cb117-22" aria-hidden="true" tabindex="-1"></a>        loss.backward()<span class="op">;</span> </span>
<span id="cb117-23"><a href="#cb117-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i<span class="op">%</span><span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> verbose: <span class="bu">print</span>(i, loss)</span>
<span id="cb117-24"><a href="#cb117-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> epochs<span class="op">-</span><span class="dv">1</span>: <span class="bu">print</span>(i, loss)</span>
<span id="cb117-25"><a href="#cb117-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-26"><a href="#cb117-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-27"><a href="#cb117-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Update / Gradient</span></span>
<span id="cb117-28"><a href="#cb117-28" aria-hidden="true" tabindex="-1"></a>        lri <span class="op">=</span> <span class="bu">next</span>(lr).item() <span class="cf">if</span> <span class="bu">hasattr</span>(lr, <span class="st">"__next__"</span>) <span class="cf">else</span> lr</span>
<span id="cb117-29"><a href="#cb117-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb117-30"><a href="#cb117-30" aria-hidden="true" tabindex="-1"></a>            p.data <span class="op">-=</span>lri<span class="op">*</span>p.grad</span>
<span id="cb117-31"><a href="#cb117-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-32"><a href="#cb117-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> tracker <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb117-33"><a href="#cb117-33" aria-hidden="true" tabindex="-1"></a>            tracker.get(<span class="st">'lr'</span>, []).append(lri)</span>
<span id="cb117-34"><a href="#cb117-34" aria-hidden="true" tabindex="-1"></a>            tracker.get(<span class="st">'loss'</span>, []).append(loss.item())</span>
<span id="cb117-35"><a href="#cb117-35" aria-hidden="true" tabindex="-1"></a>            tracker.get(<span class="st">'batch_sz'</span>, []).append(batch_sz)</span>
<span id="cb117-36"><a href="#cb117-36" aria-hidden="true" tabindex="-1"></a>            tracker.get(<span class="st">'block_sz'</span>, []).append(X.shape[<span class="dv">1</span>])</span>
<span id="cb117-37"><a href="#cb117-37" aria-hidden="true" tabindex="-1"></a>            tracker.get(<span class="st">'emb_sz'</span>, []).append(model.C.shape[<span class="dv">1</span>])</span>
<span id="cb117-38"><a href="#cb117-38" aria-hidden="true" tabindex="-1"></a>            tracker.get(<span class="st">'hidden_units'</span>, []).append(model.hidden_units)</span>
<span id="cb117-39"><a href="#cb117-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-92" class="cell">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>X.shape[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>3</code></pre>
</div>
</div>
<div id="cell-93" class="cell">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> build_XY(words, s2i, block_size<span class="op">=</span><span class="dv">3</span>)<span class="op">;</span> X.shape, Y.shape</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-94" class="cell">
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-68-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-95" class="cell">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>tracker <span class="op">=</span> {<span class="st">'lr'</span>:[], <span class="st">'batch_sz'</span>:[], <span class="st">'loss'</span>:[], <span class="st">'block_sz'</span>:[], <span class="st">'emb_sz'</span>:[], <span class="st">'hidden_units'</span>:[] }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-96" class="cell">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train(model, X, Y,  epochs<span class="op">=</span><span class="dv">400</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker, batch_sz<span class="op">=</span><span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(16.1822, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(3.1182, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(2.7789, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(2.6775, grad_fn=&lt;NllLossBackward0&gt;)
399 tensor(2.6257, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-97" class="cell">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>tracker <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>True</code></pre>
</div>
</div>
<div id="cell-98" class="cell">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>tracker.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>dict_keys(['lr', 'batch_sz', 'loss', 'block_sz', 'emb_sz', 'hidden_units'])</code></pre>
</div>
</div>
<div id="cell-99" class="cell">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>plt.plot(tracker[<span class="st">'loss'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-73-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-100" class="cell">
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-74-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="training-with-minibatch" class="level3">
<h3 class="anchored" data-anchor-id="training-with-minibatch">Training with minibatch</h3>
<div id="cell-102" class="cell">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> build_XY(words, s2i, block_size<span class="op">=</span><span class="dv">3</span>)<span class="op">;</span> X.shape, Y.shape</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i))</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-75-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-103" class="cell">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>tracker <span class="op">=</span> {<span class="st">'lr'</span>:[], <span class="st">'batch_sz'</span>:[], <span class="st">'loss'</span>:[], <span class="st">'block_sz'</span>:[], <span class="st">'emb_sz'</span>:[], <span class="st">'hidden_units'</span>:[] }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-104" class="cell">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train(model, X, Y,  epochs<span class="op">=</span><span class="dv">2000</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker, batch_sz<span class="op">=</span><span class="dv">32</span>)<span class="op">;</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(17.3736, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(3.5120, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(3.2923, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(2.5880, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(2.7083, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(2.8158, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(2.8590, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(2.7595, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(2.6768, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(2.9124, grad_fn=&lt;NllLossBackward0&gt;)
1000 tensor(2.6611, grad_fn=&lt;NllLossBackward0&gt;)
1100 tensor(2.2173, grad_fn=&lt;NllLossBackward0&gt;)
1200 tensor(2.2574, grad_fn=&lt;NllLossBackward0&gt;)
1300 tensor(2.3949, grad_fn=&lt;NllLossBackward0&gt;)
1400 tensor(2.7110, grad_fn=&lt;NllLossBackward0&gt;)
1500 tensor(2.9206, grad_fn=&lt;NllLossBackward0&gt;)
1600 tensor(2.5418, grad_fn=&lt;NllLossBackward0&gt;)
1700 tensor(2.5544, grad_fn=&lt;NllLossBackward0&gt;)
1800 tensor(2.8429, grad_fn=&lt;NllLossBackward0&gt;)
1900 tensor(2.2609, grad_fn=&lt;NllLossBackward0&gt;)
1999 tensor(2.5404, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-105" class="cell">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>model.num_params() <span class="co"># Number of parameter in the model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>3481</code></pre>
</div>
</div>
<div id="cell-106" class="cell">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>plt.plot(tracker[<span class="st">'loss'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-79-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-107" class="cell">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-80-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="implementing-word-generator" class="level3">
<h3 class="anchored" data-anchor-id="implementing-word-generator">Implementing Word Generator</h3>
<div id="cell-109" class="cell">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="co"># X[ix]</span></span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>ix <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model.C[X[ix]]</span></span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>X[ix]</span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a><span class="co"># model(torch.tensor([0,0,0]))</span></span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> model.C[torch.tensor([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>])]</span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a><span class="co"># emb.shape[0], emb.shape[1], emb.shape[2]</span></span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a>emb.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([3, 2])</code></pre>
</div>
</div>
<div id="cell-110" class="cell">
<div class="sourceCode cell-code" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a>X[ix]</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a>X.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([228146, 3])</code></pre>
</div>
</div>
<div id="cell-111" class="cell">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>ix <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>g<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a>gen_words <span class="op">=</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-112" class="cell">
<div class="sourceCode cell-code" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a>    gen_word <span class="op">=</span> <span class="st">""</span></span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>    inp <span class="op">=</span> [ix]<span class="op">*</span>model.blck_sz</span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(torch.tensor([inp]))</span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a>        counts <span class="op">=</span> logits.exp() <span class="co"># equivalent N</span></span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> counts<span class="op">/</span> counts.<span class="bu">sum</span>(<span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb145-10"><a href="#cb145-10" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb145-11"><a href="#cb145-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb145-12"><a href="#cb145-12" aria-hidden="true" tabindex="-1"></a>        inp.pop(<span class="dv">0</span>)</span>
<span id="cb145-13"><a href="#cb145-13" aria-hidden="true" tabindex="-1"></a>        inp.append(ix)</span>
<span id="cb145-14"><a href="#cb145-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>: <span class="cf">break</span></span>
<span id="cb145-15"><a href="#cb145-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: gen_word <span class="op">+=</span>i2s[ix]</span>
<span id="cb145-16"><a href="#cb145-16" aria-hidden="true" tabindex="-1"></a>    gen_words.append(gen_word)</span>
<span id="cb145-17"><a href="#cb145-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb145-18"><a href="#cb145-18" aria-hidden="true" tabindex="-1"></a><span class="co"># probs</span></span>
<span id="cb145-19"><a href="#cb145-19" aria-hidden="true" tabindex="-1"></a>ix</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0</code></pre>
</div>
</div>
<div id="cell-113" class="cell">
<div class="sourceCode cell-code" id="cb147"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a>F</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;module 'torch.nn.functional' from '/opt/homebrew/Caskroom/miniforge/base/envs/aiking/lib/python3.9/site-packages/torch/nn/functional.py'&gt;</code></pre>
</div>
</div>
<div id="cell-114" class="cell">
<div class="sourceCode cell-code" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>inp, words</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>([18, 21, 0],
 ['emma',
  'olivia',
  'ava',
  'isabella',
  'sophia',
  'charlotte',
  'mia',
  'amelia',
  'harper',
  'evelyn',
  'abigail',
  'emily',
  'elizabeth',
  'mila',
  'ella',
  'avery',
  'sofia',
  'camila',
  'aria',
  'scarlett',
  'victoria',
  'madison',
  'luna',
  'grace',
  'chloe',
  'penelope',
  'layla',
  'riley',
  'zoey',
  'nora',
  'lily',
  'eleanor',
  'hannah',
  'lillian',
  'addison',
  'aubrey',
  'ellie',
  'stella',
  'natalie',
  'zoe',
  'leah',
  'hazel',
  'violet',
  'aurora',
  'savannah',
  'audrey',
  'brooklyn',
  'bella',
  'claire',
  'skylar',
  'lucy',
  'paisley',
  'everly',
  'anna',
  'caroline',
  'nova',
  'genesis',
  'emilia',
  'kennedy',
  'samantha',
  'maya',
  'willow',
  'kinsley',
  'naomi',
  'aaliyah',
  'elena',
  'sarah',
  'ariana',
  'allison',
  'gabriella',
  'alice',
  'madelyn',
  'cora',
  'ruby',
  'eva',
  'serenity',
  'autumn',
  'adeline',
  'hailey',
  'gianna',
  'valentina',
  'isla',
  'eliana',
  'quinn',
  'nevaeh',
  'ivy',
  'sadie',
  'piper',
  'lydia',
  'alexa',
  'josephine',
  'emery',
  'julia',
  'delilah',
  'arianna',
  'vivian',
  'kaylee',
  'sophie',
  'brielle',
  'madeline',
  'peyton',
  'rylee',
  'clara',
  'hadley',
  'melanie',
  'mackenzie',
  'reagan',
  'adalynn',
  'liliana',
  'aubree',
  'jade',
  'katherine',
  'isabelle',
  'natalia',
  'raelynn',
  'maria',
  'athena',
  'ximena',
  'arya',
  'leilani',
  'taylor',
  'faith',
  'rose',
  'kylie',
  'alexandra',
  'mary',
  'margaret',
  'lyla',
  'ashley',
  'amaya',
  'eliza',
  'brianna',
  'bailey',
  'andrea',
  'khloe',
  'jasmine',
  'melody',
  'iris',
  'isabel',
  'norah',
  'annabelle',
  'valeria',
  'emerson',
  'adalyn',
  'ryleigh',
  'eden',
  'emersyn',
  'anastasia',
  'kayla',
  'alyssa',
  'juliana',
  'charlie',
  'esther',
  'ariel',
  'cecilia',
  'valerie',
  'alina',
  'molly',
  'reese',
  'aliyah',
  'lilly',
  'parker',
  'finley',
  'morgan',
  'sydney',
  'jordyn',
  'eloise',
  'trinity',
  'daisy',
  'kimberly',
  'lauren',
  'genevieve',
  'sara',
  'arabella',
  'harmony',
  'elise',
  'remi',
  'teagan',
  'alexis',
  'london',
  'sloane',
  'laila',
  'lucia',
  'diana',
  'juliette',
  'sienna',
  'elliana',
  'londyn',
  'ayla',
  'callie',
  'gracie',
  'josie',
  'amara',
  'jocelyn',
  'daniela',
  'everleigh',
  'mya',
  'rachel',
  'summer',
  'alana',
  'brooke',
  'alaina',
  'mckenzie',
  'catherine',
  'amy',
  'presley',
  'journee',
  'rosalie',
  'ember',
  'brynlee',
  'rowan',
  'joanna',
  'paige',
  'rebecca',
  'ana',
  'sawyer',
  'mariah',
  'nicole',
  'brooklynn',
  'payton',
  'marley',
  'fiona',
  'georgia',
  'lila',
  'harley',
  'adelyn',
  'alivia',
  'noelle',
  'gemma',
  'vanessa',
  'journey',
  'makayla',
  'angelina',
  'adaline',
  'catalina',
  'alayna',
  'julianna',
  'leila',
  'lola',
  'adriana',
  'june',
  'juliet',
  'jayla',
  'river',
  'tessa',
  'lia',
  'dakota',
  'delaney',
  'selena',
  'blakely',
  'ada',
  'camille',
  'zara',
  'malia',
  'hope',
  'samara',
  'vera',
  'mckenna',
  'briella',
  'izabella',
  'hayden',
  'raegan',
  'michelle',
  'angela',
  'ruth',
  'freya',
  'kamila',
  'vivienne',
  'aspen',
  'olive',
  'kendall',
  'elaina',
  'thea',
  'kali',
  'destiny',
  'amiyah',
  'evangeline',
  'cali',
  'blake',
  'elsie',
  'juniper',
  'alexandria',
  'myla',
  'ariella',
  'kate',
  'mariana',
  'lilah',
  'charlee',
  'daleyza',
  'nyla',
  'jane',
  'maggie',
  'zuri',
  'aniyah',
  'lucille',
  'leia',
  'melissa',
  'adelaide',
  'amina',
  'giselle',
  'lena',
  'camilla',
  'miriam',
  'millie',
  'brynn',
  'gabrielle',
  'sage',
  'annie',
  'logan',
  'lilliana',
  'haven',
  'jessica',
  'kaia',
  'magnolia',
  'amira',
  'adelynn',
  'makenzie',
  'stephanie',
  'nina',
  'phoebe',
  'arielle',
  'evie',
  'lyric',
  'alessandra',
  'gabriela',
  'paislee',
  'raelyn',
  'madilyn',
  'paris',
  'makenna',
  'kinley',
  'gracelyn',
  'talia',
  'maeve',
  'rylie',
  'kiara',
  'evelynn',
  'brinley',
  'jacqueline',
  'laura',
  'gracelynn',
  'lexi',
  'ariah',
  'fatima',
  'jennifer',
  'kehlani',
  'alani',
  'ariyah',
  'luciana',
  'allie',
  'heidi',
  'maci',
  'phoenix',
  'felicity',
  'joy',
  'kenzie',
  'veronica',
  'margot',
  'addilyn',
  'lana',
  'cassidy',
  'remington',
  'saylor',
  'ryan',
  'keira',
  'harlow',
  'miranda',
  'angel',
  'amanda',
  'daniella',
  'royalty',
  'gwendolyn',
  'ophelia',
  'heaven',
  'jordan',
  'madeleine',
  'esmeralda',
  'kira',
  'miracle',
  'elle',
  'amari',
  'danielle',
  'daphne',
  'willa',
  'haley',
  'gia',
  'kaitlyn',
  'oakley',
  'kailani',
  'winter',
  'alicia',
  'serena',
  'nadia',
  'aviana',
  'demi',
  'jada',
  'braelynn',
  'dylan',
  'ainsley',
  'alison',
  'camryn',
  'avianna',
  'bianca',
  'skyler',
  'scarlet',
  'maddison',
  'nylah',
  'sarai',
  'regina',
  'dahlia',
  'nayeli',
  'raven',
  'helen',
  'adrianna',
  'averie',
  'skye',
  'kelsey',
  'tatum',
  'kensley',
  'maliyah',
  'erin',
  'viviana',
  'jenna',
  'anaya',
  'carolina',
  'shelby',
  'sabrina',
  'mikayla',
  'annalise',
  'octavia',
  'lennon',
  'blair',
  'carmen',
  'yaretzi',
  'kennedi',
  'mabel',
  'zariah',
  'kyla',
  'christina',
  'selah',
  'celeste',
  'eve',
  'mckinley',
  'milani',
  'frances',
  'jimena',
  'kylee',
  'leighton',
  'katie',
  'aitana',
  'kayleigh',
  'sierra',
  'kathryn',
  'rosemary',
  'jolene',
  'alondra',
  'elisa',
  'helena',
  'charleigh',
  'hallie',
  'lainey',
  'avah',
  'jazlyn',
  'kamryn',
  'mira',
  'cheyenne',
  'francesca',
  'antonella',
  'wren',
  'chelsea',
  'amber',
  'emory',
  'lorelei',
  'nia',
  'abby',
  'april',
  'emelia',
  'carter',
  'aylin',
  'cataleya',
  'bethany',
  'marlee',
  'carly',
  'kaylani',
  'emely',
  'liana',
  'madelynn',
  'cadence',
  'matilda',
  'sylvia',
  'myra',
  'fernanda',
  'oaklyn',
  'elianna',
  'hattie',
  'dayana',
  'kendra',
  'maisie',
  'malaysia',
  'kara',
  'katelyn',
  'maia',
  'celine',
  'cameron',
  'renata',
  'jayleen',
  'charli',
  'emmalyn',
  'holly',
  'azalea',
  'leona',
  'alejandra',
  'bristol',
  'collins',
  'imani',
  'meadow',
  'alexia',
  'edith',
  'kaydence',
  'leslie',
  'lilith',
  'kora',
  'aisha',
  'meredith',
  'danna',
  'wynter',
  'emberly',
  'julieta',
  'michaela',
  'alayah',
  'jemma',
  'reign',
  'colette',
  'kaliyah',
  'elliott',
  'johanna',
  'remy',
  'sutton',
  'emmy',
  'virginia',
  'briana',
  'oaklynn',
  'adelina',
  'everlee',
  'megan',
  'angelica',
  'justice',
  'mariam',
  'khaleesi',
  'macie',
  'karsyn',
  'alanna',
  'aleah',
  'mae',
  'mallory',
  'esme',
  'skyla',
  'madilynn',
  'charley',
  'allyson',
  'hanna',
  'shiloh',
  'henley',
  'macy',
  'maryam',
  'ivanna',
  'ashlynn',
  'lorelai',
  'amora',
  'ashlyn',
  'sasha',
  'baylee',
  'beatrice',
  'itzel',
  'priscilla',
  'marie',
  'jayda',
  'liberty',
  'rory',
  'alessia',
  'alaia',
  'janelle',
  'kalani',
  'gloria',
  'sloan',
  'dorothy',
  'greta',
  'julie',
  'zahra',
  'savanna',
  'annabella',
  'poppy',
  'amalia',
  'zaylee',
  'cecelia',
  'coraline',
  'kimber',
  'emmie',
  'anne',
  'karina',
  'kassidy',
  'kynlee',
  'monroe',
  'anahi',
  'jaliyah',
  'jazmin',
  'maren',
  'monica',
  'siena',
  'marilyn',
  'reyna',
  'kyra',
  'lilian',
  'jamie',
  'melany',
  'alaya',
  'ariya',
  'kelly',
  'rosie',
  'adley',
  'dream',
  'jaylah',
  'laurel',
  'jazmine',
  'mina',
  'karla',
  'bailee',
  'aubrie',
  'katalina',
  'melina',
  'harlee',
  'elliot',
  'hayley',
  'elaine',
  'karen',
  'dallas',
  'irene',
  'lylah',
  'ivory',
  'chaya',
  'rosa',
  'aleena',
  'braelyn',
  'nola',
  'alma',
  'leyla',
  'pearl',
  'addyson',
  'roselyn',
  'lacey',
  'lennox',
  'reina',
  'aurelia',
  'noa',
  'janiyah',
  'jessie',
  'madisyn',
  'saige',
  'alia',
  'tiana',
  'astrid',
  'cassandra',
  'kyleigh',
  'romina',
  'stevie',
  'haylee',
  'zelda',
  'lillie',
  'aileen',
  'brylee',
  'eileen',
  'yara',
  'ensley',
  'lauryn',
  'giuliana',
  'livia',
  'anya',
  'mikaela',
  'palmer',
  'lyra',
  'mara',
  'marina',
  'kailey',
  'liv',
  'clementine',
  'kenna',
  'briar',
  'emerie',
  'galilea',
  'tiffany',
  'bonnie',
  'elyse',
  'cynthia',
  'frida',
  'kinslee',
  'tatiana',
  'joelle',
  'armani',
  'jolie',
  'nalani',
  'rayna',
  'yareli',
  'meghan',
  'rebekah',
  'addilynn',
  'faye',
  'zariyah',
  'lea',
  'aliza',
  'julissa',
  'lilyana',
  'anika',
  'kairi',
  'aniya',
  'noemi',
  'angie',
  'crystal',
  'bridget',
  'ari',
  'davina',
  'amelie',
  'amirah',
  'annika',
  'elora',
  'xiomara',
  'linda',
  'hana',
  'laney',
  'mercy',
  'hadassah',
  'madalyn',
  'louisa',
  'simone',
  'kori',
  'jillian',
  'alena',
  'malaya',
  'miley',
  'milan',
  'sariyah',
  'malani',
  'clarissa',
  'nala',
  'princess',
  'amani',
  'analia',
  'estella',
  'milana',
  'aya',
  'chana',
  'jayde',
  'tenley',
  'zaria',
  'itzayana',
  'penny',
  'ailani',
  'lara',
  'aubriella',
  'clare',
  'lina',
  'rhea',
  'bria',
  'thalia',
  'keyla',
  'haisley',
  'ryann',
  'addisyn',
  'amaia',
  'chanel',
  'ellen',
  'harmoni',
  'aliana',
  'tinsley',
  'landry',
  'paisleigh',
  'lexie',
  'myah',
  'rylan',
  'deborah',
  'emilee',
  'laylah',
  'novalee',
  'ellis',
  'emmeline',
  'avalynn',
  'hadlee',
  'legacy',
  'braylee',
  'elisabeth',
  'kaylie',
  'ansley',
  'dior',
  'paula',
  'belen',
  'corinne',
  'maleah',
  'martha',
  'teresa',
  'salma',
  'louise',
  'averi',
  'lilianna',
  'amiya',
  'milena',
  'royal',
  'aubrielle',
  'calliope',
  'frankie',
  'natasha',
  'kamilah',
  'meilani',
  'raina',
  'amayah',
  'lailah',
  'rayne',
  'zaniyah',
  'isabela',
  'nathalie',
  'miah',
  'opal',
  'kenia',
  'azariah',
  'hunter',
  'tori',
  'andi',
  'keily',
  'leanna',
  'scarlette',
  'jaelyn',
  'saoirse',
  'selene',
  'dalary',
  'lindsey',
  'marianna',
  'ramona',
  'estelle',
  'giovanna',
  'holland',
  'nancy',
  'emmalynn',
  'mylah',
  'rosalee',
  'sariah',
  'zoie',
  'blaire',
  'lyanna',
  'maxine',
  'anais',
  'dana',
  'judith',
  'kiera',
  'jaelynn',
  'noor',
  'kai',
  'adalee',
  'oaklee',
  'amaris',
  'jaycee',
  'belle',
  'carolyn',
  'della',
  'karter',
  'sky',
  'treasure',
  'vienna',
  'jewel',
  'rivka',
  'rosalyn',
  'alannah',
  'ellianna',
  'sunny',
  'claudia',
  'cara',
  'hailee',
  'estrella',
  'harleigh',
  'zhavia',
  'alianna',
  'brittany',
  'jaylene',
  'journi',
  'marissa',
  'mavis',
  'iliana',
  'jurnee',
  'aislinn',
  'alyson',
  'elsa',
  'kamiyah',
  'kiana',
  'lisa',
  'arlette',
  'kadence',
  'kathleen',
  'halle',
  'erika',
  'sylvie',
  'adele',
  'erica',
  'veda',
  'whitney',
  'bexley',
  'emmaline',
  'guadalupe',
  'august',
  'brynleigh',
  'gwen',
  'promise',
  'alisson',
  'india',
  'madalynn',
  'paloma',
  'patricia',
  'samira',
  'aliya',
  'casey',
  'jazlynn',
  'paulina',
  'dulce',
  'kallie',
  'perla',
  'adrienne',
  'alora',
  'nataly',
  'ayleen',
  'christine',
  'kaiya',
  'ariadne',
  'karlee',
  'barbara',
  'lillianna',
  'raquel',
  'saniyah',
  'yamileth',
  'arely',
  'celia',
  'heavenly',
  'kaylin',
  'marisol',
  'marleigh',
  'avalyn',
  'berkley',
  'kataleya',
  'zainab',
  'dani',
  'egypt',
  'joyce',
  'kenley',
  'annabel',
  'kaelyn',
  'etta',
  'hadleigh',
  'joselyn',
  'luella',
  'jaylee',
  'zola',
  'alisha',
  'ezra',
  'queen',
  'amia',
  'annalee',
  'bellamy',
  'paola',
  'tinley',
  'violeta',
  'jenesis',
  'arden',
  'giana',
  'wendy',
  'ellison',
  'florence',
  'margo',
  'naya',
  'robin',
  'sandra',
  'scout',
  'waverly',
  'janessa',
  'jayden',
  'micah',
  'novah',
  'zora',
  'ann',
  'jana',
  'taliyah',
  'vada',
  'giavanna',
  'ingrid',
  'valery',
  'azaria',
  'emmarie',
  'esperanza',
  'kailyn',
  'aiyana',
  'keilani',
  'austyn',
  'whitley',
  'elina',
  'kimora',
  'maliah',
  ...])</code></pre>
</div>
</div>
<hr>
<p><a href="https://github.com/Rahuketu86/minion/blob/main/minion/makemore/mlp.py#L141" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="gen_word_nn" class="level3">
<h3 class="anchored" data-anchor-id="gen_word_nn">gen_word_nn</h3>
<blockquote class="blockquote">
<pre><code> gen_word_nn (model, i2s, n_samples=20, g=&lt;torch._C.Generator object at
              0x7f7708e21010&gt;, logit2prob=&lt;function softmax&gt;)</code></pre>
</blockquote>
<div id="cell-116" class="cell">
<details open="" class="code-fold">
<summary>Exported source</summary>
<div class="sourceCode cell-code" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gen_word_nn(model, i2s, n_samples<span class="op">=</span><span class="dv">20</span>, g<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">2147483647</span>), logit2prob<span class="op">=</span>F.softmax):</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>    gen_words <span class="op">=</span> []</span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb152-5"><a href="#cb152-5" aria-hidden="true" tabindex="-1"></a>        gen_word <span class="op">=</span> <span class="st">""</span></span>
<span id="cb152-6"><a href="#cb152-6" aria-hidden="true" tabindex="-1"></a>        inp <span class="op">=</span> [ix]<span class="op">*</span>model.blck_sz</span>
<span id="cb152-7"><a href="#cb152-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb152-8"><a href="#cb152-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb152-9"><a href="#cb152-9" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> model(torch.tensor([inp]))</span>
<span id="cb152-10"><a href="#cb152-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># counts = logits.exp() # equivalent N</span></span>
<span id="cb152-11"><a href="#cb152-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># probs = counts/ counts.sum(1, keepdims=True)</span></span>
<span id="cb152-12"><a href="#cb152-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-13"><a href="#cb152-13" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> logit2prob(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb152-14"><a href="#cb152-14" aria-hidden="true" tabindex="-1"></a>            ix <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb152-15"><a href="#cb152-15" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb152-16"><a href="#cb152-16" aria-hidden="true" tabindex="-1"></a>            inp.pop(<span class="dv">0</span>)</span>
<span id="cb152-17"><a href="#cb152-17" aria-hidden="true" tabindex="-1"></a>            inp.append(ix)</span>
<span id="cb152-18"><a href="#cb152-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>: <span class="cf">break</span></span>
<span id="cb152-19"><a href="#cb152-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>: gen_word <span class="op">+=</span>i2s[ix]</span>
<span id="cb152-20"><a href="#cb152-20" aria-hidden="true" tabindex="-1"></a>        gen_words.append(gen_word)</span>
<span id="cb152-21"><a href="#cb152-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gen_words</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-117" class="cell">
<div class="sourceCode cell-code" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>gen_word_nn(model, i2s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['tex',
 'maloallurarlen',
 'tynn',
 'ralimittarnrllayn',
 'kanda',
 'raciynu',
 'jarhrygotei',
 'molielltiu',
 'jerteda',
 'kareynm',
 'sadlu',
 'nkaviyn',
 'rytlsp',
 'hulinnnvtarlysu',
 'jsdr',
 'ban',
 'jlhpynw',
 'iranl',
 'raldynleez',
 'myderu']</code></pre>
</div>
</div>
</section>
</section>
<section id="hyperparameter-tuning" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter tuning</h2>
<section id="how-do-we-determine-the-learning-rate-and-gain-confidence-that-we-are-stepping-the-right-speed" class="level3">
<h3 class="anchored" data-anchor-id="how-do-we-determine-the-learning-rate-and-gain-confidence-that-we-are-stepping-the-right-speed">How do we determine the learning rate and gain confidence that we are stepping the right speed?</h3>
<p>Typical steps in learning rate optimization may include</p>
<ul>
<li>Finding the range with lower limit of <code>lr</code> where loss doesn’t change much and higher limit where loss completely explodes</li>
<li>Finding the more appropriate range of learning rate by choosing an exponentiation scheme and running the training for few epochs</li>
<li>Running training for lots of epochs on choosen <code>lr</code> ( We should continue to do it till learning rate keeps decreasing signficantly)</li>
<li>Decreasing <code>lr</code> by a factor of 10 and continue to train further to get most optimized learning rate.</li>
</ul>
<div id="cell-121" class="cell">
<div class="sourceCode cell-code" id="cb155"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> build_XY(words, s2i, block_size<span class="op">=</span><span class="dv">3</span>)<span class="op">;</span> X.shape, Y.shape</span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i))</span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-90-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-122" class="cell">
<div class="sourceCode cell-code" id="cb156"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>F.cross_entropy(model(X), Y) <span class="co"># Initial loss without any optimization</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(15.4238, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-123" class="cell">
<div class="sourceCode cell-code" id="cb158"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a>train(model, X, Y, lr<span class="op">=</span><span class="fl">0.0001</span>, epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(18.4698, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(17.9110, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(16.4214, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(14.0824, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(13.0391, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(16.9305, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(16.5712, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(15.7921, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(12.6845, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(12.2210, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(17.0267, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-124" class="cell">
<div class="sourceCode cell-code" id="cb161"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>train(model, X, Y, lr<span class="op">=</span><span class="dv">1</span>, epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(13.1564, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(8.6120, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(4.7459, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(5.8324, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(5.4408, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(5.1435, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(4.8215, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(4.8802, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(3.4567, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(3.2230, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(3.2829, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-125" class="cell">
<div class="sourceCode cell-code" id="cb164"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a>train(model, X, Y, lr<span class="op">=</span><span class="dv">10</span>, epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(2.8849, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(83.2786, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(93.7002, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(60.7121, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(34.8946, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(79.4215, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(34.4919, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(54.7734, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(58.9727, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(54.2846, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(76.2547, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<p>This indicates learning rate should be choosen between 0 and 1</p>
<hr>
<p><a href="https://github.com/Rahuketu86/minion/blob/main/minion/makemore/mlp.py#L164" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="lr_scheduler" class="level3">
<h3 class="anchored" data-anchor-id="lr_scheduler">lr_scheduler</h3>
<blockquote class="blockquote">
<pre><code> lr_scheduler (low=-3, upper=0, steps=1000)</code></pre>
</blockquote>
<div id="cell-128" class="cell">
<details open="" class="code-fold">
<summary>Exported source</summary>
<div class="sourceCode cell-code" id="cb168"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lr_scheduler(low<span class="op">=-</span><span class="dv">3</span>, upper<span class="op">=</span><span class="dv">0</span>, steps<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb168-2"><a href="#cb168-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">yield</span> <span class="cf">from</span> <span class="dv">10</span><span class="op">**</span>torch.linspace(low, upper, steps)</span>
<span id="cb168-3"><a href="#cb168-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-4"><a href="#cb168-4" aria-hidden="true" tabindex="-1"></a><span class="co"># for i in lr_scheduler():</span></span>
<span id="cb168-5"><a href="#cb168-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(i)</span></span>
<span id="cb168-6"><a href="#cb168-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># break</span></span>
<span id="cb168-7"><a href="#cb168-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-8"><a href="#cb168-8" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.linspace(-3, 0, 1000)</span></span>
<span id="cb168-9"><a href="#cb168-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-10"><a href="#cb168-10" aria-hidden="true" tabindex="-1"></a><span class="co"># for i in torch.pow(10, torch.linspace(-3, 0, 1000)):</span></span>
<span id="cb168-11"><a href="#cb168-11" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(i)</span></span>
<span id="cb168-12"><a href="#cb168-12" aria-hidden="true" tabindex="-1"></a><span class="co"># for i in torch.pow(10, torch.linspace(-3, 0, 1000)):</span></span>
<span id="cb168-13"><a href="#cb168-13" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(i.item())</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-129" class="cell">
<div class="sourceCode cell-code" id="cb169"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="bu">hasattr</span>( lr_scheduler(), <span class="st">"__next__"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>True</code></pre>
</div>
</div>
<div id="cell-130" class="cell">
<div class="sourceCode cell-code" id="cb171"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="bu">next</span>(lr_scheduler())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.0010)</code></pre>
</div>
</div>
<div id="cell-131" class="cell">
<div class="sourceCode cell-code" id="cb173"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a>tracker <span class="op">=</span> {<span class="st">'lr'</span>:[], <span class="st">'batch_sz'</span>:[], <span class="st">'loss'</span>:[], <span class="st">'block_sz'</span>:[], <span class="st">'emb_sz'</span>:[], <span class="st">'hidden_units'</span>:[] }</span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i))</span>
<span id="cb173-3"><a href="#cb173-3" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)<span class="op">;</span> F.cross_entropy(model(X), Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(19.4207, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-99-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-132" class="cell">
<div class="sourceCode cell-code" id="cb175"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a>train(model, X, Y, lr<span class="op">=</span>lr_scheduler(steps<span class="op">=</span><span class="dv">1000</span>), epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(20.5248, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(17.5990, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(13.2811, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(7.4093, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(6.2343, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(3.3622, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(3.9428, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(3.1582, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(3.4537, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(6.5724, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(7.3060, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-133" class="cell">
<div class="sourceCode cell-code" id="cb178"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a>plt.plot(tracker[<span class="st">'lr'</span>], tracker[<span class="st">'loss'</span>])</span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a><span class="co"># tracker['lr']</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-101-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-134" class="cell">
<div class="sourceCode cell-code" id="cb179"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a>min_idx <span class="op">=</span> torch.tensor(tracker[<span class="st">'loss'</span>]).<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).indices.item()</span>
<span id="cb179-2"><a href="#cb179-2" aria-hidden="true" tabindex="-1"></a>tracker[<span class="st">'lr'</span>][min_idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.04804869741201401</code></pre>
</div>
</div>
<div id="cell-135" class="cell">
<div class="sourceCode cell-code" id="cb181"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a>lrs <span class="op">=</span> torch.tensor(tracker[<span class="st">'lr'</span>])</span>
<span id="cb181-2"><a href="#cb181-2" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> torch.tensor(tracker[<span class="st">'loss'</span>])</span>
<span id="cb181-3"><a href="#cb181-3" aria-hidden="true" tabindex="-1"></a>log_lrs <span class="op">=</span> lrs.log()</span>
<span id="cb181-4"><a href="#cb181-4" aria-hidden="true" tabindex="-1"></a>log_lrs[min_idx], lrs[min_idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(-3.0355), tensor(0.0480))</code></pre>
</div>
</div>
<div id="cell-136" class="cell">
<div class="sourceCode cell-code" id="cb183"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a>plt.plot(log_lrs, tracker[<span class="st">'loss'</span>])</span>
<span id="cb183-2"><a href="#cb183-2" aria-hidden="true" tabindex="-1"></a>plt.axvline(log_lrs[min_idx], color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb183-3"><a href="#cb183-3" aria-hidden="true" tabindex="-1"></a>plt.text(log_lrs[min_idx]<span class="op">*</span><span class="fl">0.9</span>,losses[min_idx]<span class="op">*</span><span class="fl">0.9</span>, <span class="ss">f"lr : </span><span class="sc">{</span>lrs[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">, loss : </span><span class="sc">{</span>losses[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(tensor(-2.7320), tensor(2.1892), 'lr : 0.05, loss : 2.43')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-104-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-137" class="cell">
<div class="sourceCode cell-code" id="cb185"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a>train(model, X, Y, lr<span class="op">=</span><span class="fl">0.05</span>, epochs<span class="op">=</span><span class="dv">10000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(7.7239, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(3.4564, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(2.3491, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(2.6205, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(3.1440, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(2.7919, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(2.4809, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(2.8206, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(2.6108, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(2.8789, grad_fn=&lt;NllLossBackward0&gt;)
1000 tensor(2.2781, grad_fn=&lt;NllLossBackward0&gt;)
1100 tensor(2.3718, grad_fn=&lt;NllLossBackward0&gt;)
1200 tensor(3.1925, grad_fn=&lt;NllLossBackward0&gt;)
1300 tensor(2.6590, grad_fn=&lt;NllLossBackward0&gt;)
1400 tensor(2.4634, grad_fn=&lt;NllLossBackward0&gt;)
1500 tensor(2.5142, grad_fn=&lt;NllLossBackward0&gt;)
1600 tensor(2.6078, grad_fn=&lt;NllLossBackward0&gt;)
1700 tensor(2.4267, grad_fn=&lt;NllLossBackward0&gt;)
1800 tensor(2.3398, grad_fn=&lt;NllLossBackward0&gt;)
1900 tensor(2.1698, grad_fn=&lt;NllLossBackward0&gt;)
2000 tensor(2.4727, grad_fn=&lt;NllLossBackward0&gt;)
2100 tensor(2.6651, grad_fn=&lt;NllLossBackward0&gt;)
2200 tensor(2.8141, grad_fn=&lt;NllLossBackward0&gt;)
2300 tensor(2.6341, grad_fn=&lt;NllLossBackward0&gt;)
2400 tensor(2.4364, grad_fn=&lt;NllLossBackward0&gt;)
2500 tensor(2.5922, grad_fn=&lt;NllLossBackward0&gt;)
2600 tensor(2.5449, grad_fn=&lt;NllLossBackward0&gt;)
2700 tensor(2.6220, grad_fn=&lt;NllLossBackward0&gt;)
2800 tensor(2.1987, grad_fn=&lt;NllLossBackward0&gt;)
2900 tensor(2.3869, grad_fn=&lt;NllLossBackward0&gt;)
3000 tensor(2.3194, grad_fn=&lt;NllLossBackward0&gt;)
3100 tensor(2.1561, grad_fn=&lt;NllLossBackward0&gt;)
3200 tensor(2.2904, grad_fn=&lt;NllLossBackward0&gt;)
3300 tensor(2.7685, grad_fn=&lt;NllLossBackward0&gt;)
3400 tensor(2.4733, grad_fn=&lt;NllLossBackward0&gt;)
3500 tensor(2.3755, grad_fn=&lt;NllLossBackward0&gt;)
3600 tensor(2.3150, grad_fn=&lt;NllLossBackward0&gt;)
3700 tensor(2.2893, grad_fn=&lt;NllLossBackward0&gt;)
3800 tensor(2.1896, grad_fn=&lt;NllLossBackward0&gt;)
3900 tensor(2.6078, grad_fn=&lt;NllLossBackward0&gt;)
4000 tensor(2.6497, grad_fn=&lt;NllLossBackward0&gt;)
4100 tensor(2.3783, grad_fn=&lt;NllLossBackward0&gt;)
4200 tensor(2.6059, grad_fn=&lt;NllLossBackward0&gt;)
4300 tensor(2.0487, grad_fn=&lt;NllLossBackward0&gt;)
4400 tensor(2.2884, grad_fn=&lt;NllLossBackward0&gt;)
4500 tensor(2.7715, grad_fn=&lt;NllLossBackward0&gt;)
4600 tensor(2.6611, grad_fn=&lt;NllLossBackward0&gt;)
4700 tensor(2.3218, grad_fn=&lt;NllLossBackward0&gt;)
4800 tensor(2.7876, grad_fn=&lt;NllLossBackward0&gt;)
4900 tensor(2.3589, grad_fn=&lt;NllLossBackward0&gt;)
5000 tensor(2.4277, grad_fn=&lt;NllLossBackward0&gt;)
5100 tensor(2.4410, grad_fn=&lt;NllLossBackward0&gt;)
5200 tensor(2.6355, grad_fn=&lt;NllLossBackward0&gt;)
5300 tensor(2.5415, grad_fn=&lt;NllLossBackward0&gt;)
5400 tensor(2.3694, grad_fn=&lt;NllLossBackward0&gt;)
5500 tensor(2.1534, grad_fn=&lt;NllLossBackward0&gt;)
5600 tensor(2.2819, grad_fn=&lt;NllLossBackward0&gt;)
5700 tensor(2.8102, grad_fn=&lt;NllLossBackward0&gt;)
5800 tensor(2.1185, grad_fn=&lt;NllLossBackward0&gt;)
5900 tensor(2.7238, grad_fn=&lt;NllLossBackward0&gt;)
6000 tensor(2.4982, grad_fn=&lt;NllLossBackward0&gt;)
6100 tensor(2.3530, grad_fn=&lt;NllLossBackward0&gt;)
6200 tensor(2.7606, grad_fn=&lt;NllLossBackward0&gt;)
6300 tensor(2.2108, grad_fn=&lt;NllLossBackward0&gt;)
6400 tensor(2.4615, grad_fn=&lt;NllLossBackward0&gt;)
6500 tensor(2.2749, grad_fn=&lt;NllLossBackward0&gt;)
6600 tensor(2.2130, grad_fn=&lt;NllLossBackward0&gt;)
6700 tensor(2.3261, grad_fn=&lt;NllLossBackward0&gt;)
6800 tensor(2.7720, grad_fn=&lt;NllLossBackward0&gt;)
6900 tensor(2.3050, grad_fn=&lt;NllLossBackward0&gt;)
7000 tensor(2.6368, grad_fn=&lt;NllLossBackward0&gt;)
7100 tensor(2.4573, grad_fn=&lt;NllLossBackward0&gt;)
7200 tensor(2.5606, grad_fn=&lt;NllLossBackward0&gt;)
7300 tensor(2.5008, grad_fn=&lt;NllLossBackward0&gt;)
7400 tensor(2.1611, grad_fn=&lt;NllLossBackward0&gt;)
7500 tensor(2.5621, grad_fn=&lt;NllLossBackward0&gt;)
7600 tensor(2.2802, grad_fn=&lt;NllLossBackward0&gt;)
7700 tensor(2.7898, grad_fn=&lt;NllLossBackward0&gt;)
7800 tensor(2.4912, grad_fn=&lt;NllLossBackward0&gt;)
7900 tensor(2.3120, grad_fn=&lt;NllLossBackward0&gt;)
8000 tensor(2.4449, grad_fn=&lt;NllLossBackward0&gt;)
8100 tensor(2.2586, grad_fn=&lt;NllLossBackward0&gt;)
8200 tensor(2.6501, grad_fn=&lt;NllLossBackward0&gt;)
8300 tensor(2.4992, grad_fn=&lt;NllLossBackward0&gt;)
8400 tensor(2.3757, grad_fn=&lt;NllLossBackward0&gt;)
8500 tensor(2.2224, grad_fn=&lt;NllLossBackward0&gt;)
8600 tensor(3.0371, grad_fn=&lt;NllLossBackward0&gt;)
8700 tensor(2.2996, grad_fn=&lt;NllLossBackward0&gt;)
8800 tensor(2.2606, grad_fn=&lt;NllLossBackward0&gt;)
8900 tensor(2.2842, grad_fn=&lt;NllLossBackward0&gt;)
9000 tensor(2.3264, grad_fn=&lt;NllLossBackward0&gt;)
9100 tensor(2.3570, grad_fn=&lt;NllLossBackward0&gt;)
9200 tensor(1.9541, grad_fn=&lt;NllLossBackward0&gt;)
9300 tensor(2.5794, grad_fn=&lt;NllLossBackward0&gt;)
9400 tensor(2.4059, grad_fn=&lt;NllLossBackward0&gt;)
9500 tensor(2.2326, grad_fn=&lt;NllLossBackward0&gt;)
9600 tensor(2.5177, grad_fn=&lt;NllLossBackward0&gt;)
9700 tensor(2.5604, grad_fn=&lt;NllLossBackward0&gt;)
9800 tensor(2.7318, grad_fn=&lt;NllLossBackward0&gt;)
9900 tensor(2.4444, grad_fn=&lt;NllLossBackward0&gt;)
9999 tensor(1.9725, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-138" class="cell">
<div class="sourceCode cell-code" id="cb188"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a>train(model, X, Y, lr<span class="op">=</span><span class="fl">0.005</span>, epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(2.4155, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(2.3920, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(2.2776, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(2.1948, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(2.7162, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(2.2897, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(2.2183, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(2.1047, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(2.0977, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(2.2447, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(2.4003, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-139" class="cell">
<div class="sourceCode cell-code" id="cb191"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Weight decay</span></span>
<span id="cb191-2"><a href="#cb191-2" aria-hidden="true" tabindex="-1"></a>train(model, X, Y, lr<span class="op">=</span><span class="fl">0.0050</span>, epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(2.2350, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(2.0883, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(2.5197, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(2.2276, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(2.6038, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(2.0107, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(2.3911, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(2.7271, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(2.7542, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(2.4474, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(2.3388, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-140" class="cell">
<div class="sourceCode cell-code" id="cb194"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb194-1"><a href="#cb194-1" aria-hidden="true" tabindex="-1"></a>plt.plot(tracker[<span class="st">'lr'</span>], tracker[<span class="st">'loss'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-108-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-141" class="cell">
<div class="sourceCode cell-code" id="cb195"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a>lrs <span class="op">=</span> torch.tensor(tracker[<span class="st">'lr'</span>])</span>
<span id="cb195-2"><a href="#cb195-2" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> torch.tensor(tracker[<span class="st">'loss'</span>])</span>
<span id="cb195-3"><a href="#cb195-3" aria-hidden="true" tabindex="-1"></a>log_lrs <span class="op">=</span> lrs.log()</span>
<span id="cb195-4"><a href="#cb195-4" aria-hidden="true" tabindex="-1"></a>plt.plot(log_lrs, tracker[<span class="st">'loss'</span>])</span>
<span id="cb195-5"><a href="#cb195-5" aria-hidden="true" tabindex="-1"></a>plt.axvline(log_lrs[min_idx], color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb195-6"><a href="#cb195-6" aria-hidden="true" tabindex="-1"></a>plt.text(log_lrs[min_idx]<span class="op">*</span><span class="fl">0.9</span>,losses[min_idx]<span class="op">*</span><span class="fl">0.9</span>, <span class="ss">f"lr : </span><span class="sc">{</span>lrs[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">, loss : </span><span class="sc">{</span>losses[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(tensor(-2.7320), tensor(2.1892), 'lr : 0.05, loss : 2.43')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-109-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-142" class="cell">
<div class="sourceCode cell-code" id="cb197"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a>plt.plot(tracker[<span class="st">'loss'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-110-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-143" class="cell">
<div class="sourceCode cell-code" id="cb198"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb198-1"><a href="#cb198-1" aria-hidden="true" tabindex="-1"></a>plt.plot(tracker[<span class="st">'loss'</span>][<span class="op">-</span><span class="dv">1000</span>:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-111-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-144" class="cell">
<div class="sourceCode cell-code" id="cb199"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-112-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Some observations</p>
<ul>
<li><code>.</code> is separated</li>
<li>vowels are clustered together</li>
</ul>
<div id="cell-146" class="cell">
<div class="sourceCode cell-code" id="cb200"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a>gen_word_nn(model, i2s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['engem',
 'les',
 'lmi',
 'avay',
 'caliysoe',
 'laiten',
 'mie',
 'gariseriyen',
 'khille',
 'nbhkie',
 'mesahsammho',
 'maxem',
 'keis',
 'cren',
 'alianhh',
 'jayzeanal',
 'selanellechay',
 'raloiad',
 'yven',
 'ilasho']</code></pre>
</div>
</div>
</section>
</section>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<section id="why-should-you-prefer-f.cross_entropy-over-rolling-your-own-implementation-like-nll-above" class="level3">
<h3 class="anchored" data-anchor-id="why-should-you-prefer-f.cross_entropy-over-rolling-your-own-implementation-like-nll-above">Why should you prefer <code>F.cross_entropy</code> over rolling your own implementation like <a href="https://Rahuketu86.github.io/minion/makemore.mlp.html#nll"><code>nll</code></a> above?</h3>
<ul>
<li><p>Memory and by extension compute efficiency on forward pass When using F.cross_entropy, Pytorch doesn’t create all the additional intermediate tensors for operations like exponentiation, division/ probability calculations and mean summing in memory. It will cluster up all the operations and very often will create a fused kernels, that very efficiently evaluate expression / like clustered mathematical operations</p></li>
<li><p>Backward pass can be made much more efficient. Not just becoz it is a fused kernel but analytically and mathematically, it is a much more simpler pass to implement.[This is similar to manually implementing <code>tanh</code> earlier in <code>micrograd</code>]</p>
<ul>
<li>We are able to reuse calculations</li>
<li>We are able to derive and simplify gradient calculations mathematically and analytics , eliminating many extra operations. With much less to implement.</li>
<li>Efficient because calculations can run in a fused kernel + expression can take a much more simpler form mathematically</li>
</ul></li>
<li><p>Under the hood <code>F.cross_entropy</code> can be significantly more well behaved numerically :-</p></li>
</ul>
<div id="cell-149" class="cell">
<div class="sourceCode cell-code" id="cb202"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">5</span>]])</span>
<span id="cb202-2"><a href="#cb202-2" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb202-3"><a href="#cb202-3" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts<span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> probs, probs.<span class="bu">sum</span>()</span>
<span id="cb202-4"><a href="#cb202-4" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = -probs[torch.arange(len(target)), target].log().mean()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[9.0466e-04, 3.3281e-04, 6.6846e-03, 9.9208e-01]]), tensor(1.))</code></pre>
</div>
</div>
<p>Above is well behaved and we get a good probability distribution. Now consider cases where sum of these logits can take on more extreme values[ A case which can often happen during optimization of a neural network]</p>
<div id="cell-151" class="cell">
<div class="sourceCode cell-code" id="cb204"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">5</span>]])</span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb204-3"><a href="#cb204-3" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts<span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> probs , probs.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[0.0000e+00, 3.3311e-04, 6.6906e-03, 9.9298e-01]]), tensor(1.))</code></pre>
</div>
</div>
<p>If some of these become extremely negative =&gt; It is still ok. We still get probabilities that are well behaved and they sum to 1.</p>
<div id="cell-153" class="cell">
<div class="sourceCode cell-code" id="cb206"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb206-1"><a href="#cb206-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">100</span>]])</span>
<span id="cb206-2"><a href="#cb206-2" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb206-3"><a href="#cb206-3" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts<span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> probs , probs.<span class="bu">sum</span>(), counts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[0., 0., 0., nan]]),
 tensor(nan),
 tensor([[3.7835e-44, 4.9787e-02, 1.0000e+00,        inf]]))</code></pre>
</div>
</div>
<p>Because of the way exponentiation work. If you have very positive logits. We get not a number or <code>nan</code> because we have an <code>inf</code> in the counts because we run out of <code>dynamic range</code> in floating point number that represent these calculations. To fix this check below</p>
<ul>
<li>Because of the normalization in <code>probs</code> we can offset logits by any constant value and still get same results</li>
</ul>
<div id="cell-155" class="cell">
<div class="sourceCode cell-code" id="cb208"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">5</span>]])</span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp()</span>
<span id="cb208-3"><a href="#cb208-3" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts<span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> probs , probs.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[0.0000e+00, 3.3311e-04, 6.6906e-03, 9.9298e-01]]), tensor(1.))</code></pre>
</div>
</div>
<div id="cell-156" class="cell">
<div class="sourceCode cell-code" id="cb210"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb210-2"><a href="#cb210-2" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">5</span>]])</span>
<span id="cb210-3"><a href="#cb210-3" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> (logits<span class="op">+</span>c).exp()</span>
<span id="cb210-4"><a href="#cb210-4" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts<span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> probs , probs.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[0.0000e+00, 3.3311e-04, 6.6906e-03, 9.9298e-01]]), tensor(1.))</code></pre>
</div>
</div>
<div id="cell-157" class="cell">
<div class="sourceCode cell-code" id="cb212"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="op">-</span><span class="dv">4</span></span>
<span id="cb212-2"><a href="#cb212-2" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">5</span>]])</span>
<span id="cb212-3"><a href="#cb212-3" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> (logits<span class="op">+</span>c).exp()</span>
<span id="cb212-4"><a href="#cb212-4" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts<span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> probs , probs.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[0.0000e+00, 3.3311e-04, 6.6906e-03, 9.9298e-01]]), tensor(1.))</code></pre>
</div>
</div>
<div id="cell-158" class="cell">
<div class="sourceCode cell-code" id="cb214"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb214-1"><a href="#cb214-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">5</span>]])</span>
<span id="cb214-2"><a href="#cb214-2" aria-hidden="true" tabindex="-1"></a>c  <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">max</span>(logits)</span>
<span id="cb214-3"><a href="#cb214-3" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> (logits<span class="op">+</span>c).exp()</span>
<span id="cb214-4"><a href="#cb214-4" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts<span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> probs , probs.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[0.0000e+00, 3.3311e-04, 6.6906e-03, 9.9298e-01]]), tensor(1.))</code></pre>
</div>
</div>
<p>We can do below as a fix because negative numbers are ok but positive numbers are not because they may overflow in exponentiation operation</p>
<div id="cell-160" class="cell">
<div class="sourceCode cell-code" id="cb216"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb216-1"><a href="#cb216-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">100</span>]])</span>
<span id="cb216-2"><a href="#cb216-2" aria-hidden="true" tabindex="-1"></a>c  <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">max</span>(logits)</span>
<span id="cb216-3"><a href="#cb216-3" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> (logits<span class="op">+</span>c).exp()</span>
<span id="cb216-4"><a href="#cb216-4" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts<span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> probs , probs.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00]]), tensor(1.))</code></pre>
</div>
</div>
</section>
<section id="why-is-minibatch-useful-how-do-you-do-it" class="level3">
<h3 class="anchored" data-anchor-id="why-is-minibatch-useful-how-do-you-do-it">Why is minibatch useful? How do you do it?</h3>
<ul>
<li>We can overfit our model with many parameters(here -&gt; 3481) a single batch of data, say of s ize 32 like above and get a very low loss very quickly We can run many many examples instantly and decrease the loss much much faster.</li>
<li>We randomly select some portion of the data and do forward pass, backward pass and update.</li>
<li>During minibatch - quality of the gradient is lower -&gt; it’s direction is not as reliable as calculating gradient on full dataset. But the gradient direction is good enough that it is useful.</li>
<li>It is much better to take approximate gradient and make more steps than to evaluate exact gradient and make fewer steps.</li>
</ul>
<div id="cell-162" class="cell">
<div class="sourceCode cell-code" id="cb218"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb218-1"><a href="#cb218-1" aria-hidden="true" tabindex="-1"></a>torch.randint(<span class="dv">0</span>, X.shape[<span class="dv">0</span>], (<span class="dv">32</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([ 29418, 217788,  87948,   2674,  30911,  38489,  91886, 112315,  46407,
        100503, 200258,  89289, 123189,   3198,  80933, 194688,  37236,  41736,
         58361, 110855,  97920,  43155, 176362, 111781,  35577, 155997, 199016,
         51131,  46479,  39425,  26168, 150284])</code></pre>
</div>
</div>
<div id="cell-163" class="cell">
<div class="sourceCode cell-code" id="cb220"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb220-1"><a href="#cb220-1" aria-hidden="true" tabindex="-1"></a>torch.randint(<span class="dv">0</span>, X.shape[<span class="dv">0</span>], (<span class="dv">32</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([205433, 174337,  24966,  86327, 134216,  24402,  60625,  37459, 188056,
        194326,  91958, 159822, 193533,  80115,   4786,  17577, 175678,  52190,
         61067, 198309, 137570, 102400, 180954, 195297, 106963, 199382, 205548,
         35175,  19009,  22273,   3802, 177915])</code></pre>
</div>
</div>
</section>
<section id="why-do-we-not-achieve-exactly-zero-loss-in-above-operation-with-32-size-5-words" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-not-achieve-exactly-zero-loss-in-above-operation-with-32-size-5-words">Why do we not achieve exactly zero loss in above operation with 32 size ( 5 words)?</h3>
<ul>
<li>In examples like below we have … resulting in either e or i or s. Similarily mma or via or hia goes to “.” . This indicates fundamental uncertainty (multiple outcomes in the training set for the same input.) in the data and therefore, loss can’t be eliminated to zero(we are not able to comp. letely overfit and make the loss exactly zero). For cases where we have unique input to unique output we are getting very close to zero.</li>
</ul>
<div id="cell-165" class="cell">
<div class="sourceCode cell-code" id="cb222"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb222-1"><a href="#cb222-1" aria-hidden="true" tabindex="-1"></a>X_sub, Y_sub <span class="op">=</span> build_XY(words[:<span class="dv">5</span>], s2i, block_size<span class="op">=</span><span class="dv">3</span>, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb222-2"><a href="#cb222-2" aria-hidden="true" tabindex="-1"></a>X_sub.shape, X_sub.dtype, Y_sub.shape, Y_sub.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>... --&gt; e
..e --&gt; m
.em --&gt; m
emm --&gt; a
mma --&gt; .
... --&gt; o
..o --&gt; l
.ol --&gt; i
oli --&gt; v
liv --&gt; i
ivi --&gt; a
via --&gt; .
... --&gt; a
..a --&gt; v
.av --&gt; a
ava --&gt; .
... --&gt; i
..i --&gt; s
.is --&gt; a
isa --&gt; b
sab --&gt; e
abe --&gt; l
bel --&gt; l
ell --&gt; a
lla --&gt; .
... --&gt; s
..s --&gt; o
.so --&gt; p
sop --&gt; h
oph --&gt; i
phi --&gt; a
hia --&gt; .</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)</code></pre>
</div>
</div>
<div id="cell-166" class="cell">
<div class="sourceCode cell-code" id="cb225"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb225-1"><a href="#cb225-1" aria-hidden="true" tabindex="-1"></a>model_sub <span class="op">=</span> Model(<span class="bu">len</span>(s2i))</span>
<span id="cb225-2"><a href="#cb225-2" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model_sub, s2i)</span>
<span id="cb225-3"><a href="#cb225-3" aria-hidden="true" tabindex="-1"></a>train(model_sub, X_sub, Y_sub)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>999 tensor(0.2561, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-125-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-167" class="cell">
<div class="sourceCode cell-code" id="cb227"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb227-1"><a href="#cb227-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> model_sub(X_sub)</span>
<span id="cb227-2"><a href="#cb227-2" aria-hidden="true" tabindex="-1"></a>logits.<span class="bu">max</span>(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.return_types.max(
values=tensor([ 8.8890, 15.0034, 14.1475, 16.4668, 17.5133,  8.8890, 13.0042, 17.1599,
        22.3864, 12.0286, 17.3560, 14.6342,  8.8890, 15.7620, 16.7767, 19.9523,
         8.8890, 16.0569, 13.5875, 17.6862, 12.0533, 13.1167, 17.7194, 16.4584,
        18.3450,  8.8890, 15.7595, 12.4653, 14.3792, 22.8371, 19.5576, 15.8851],
       grad_fn=&lt;MaxBackward0&gt;),
indices=tensor([15, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0, 15, 22,  1,  0, 15, 19,
         1,  2,  5, 12, 12,  1,  0, 15, 15, 16,  8,  9,  1,  0]))</code></pre>
</div>
</div>
<div id="cell-168" class="cell">
<div class="sourceCode cell-code" id="cb229"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb229-1"><a href="#cb229-1" aria-hidden="true" tabindex="-1"></a>Y_sub</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,
         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])</code></pre>
</div>
</div>
<div id="cell-169" class="cell">
<div class="sourceCode cell-code" id="cb231"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb231-1"><a href="#cb231-1" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model_sub, s2i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-128-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="why-do-we-need-to-split-dataset" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-need-to-split-dataset">Why do we need to split dataset?</h3>
<ul>
<li><p>As capacity of the neural network grows from 1000 - 10K to 100K parameters it becomes more and more capable of matching your dataset. Loss on training set will become very very low but it only means model is become more and more capable of memorizing your data.</p></li>
<li><p>When you take such model(overfitted) and try to sample from it , you will only get example of names which are exactly like the once in training set. It will not generate new examples of name. Loss of such model on unseen data can be very high.</p></li>
<li><p>We usually split the dataset into 3 splits</p>
<ul>
<li>Training Split (~80% here)</li>
<li>Dev/ Validation Split (~10% here) - Development for all the hyperparameters of the model(e.g.&nbsp;- Size of hidden layer, size of embeddings, Strength of the regularization)</li>
<li>Test/ Blind Split(~10% here) - Performance of the model at the end. We are only evaluating loss on Test set very very sparingly.</li>
</ul></li>
</ul>
<div id="cell-171" class="cell">
<div class="sourceCode cell-code" id="cb232"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb232-1"><a href="#cb232-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> build_XY(words, s2i, block_size<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb232-2"><a href="#cb232-2" aria-hidden="true" tabindex="-1"></a>X.shape, Y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([228146, 3]), torch.Size([228146]))</code></pre>
</div>
</div>
<div id="cell-172" class="cell">
<div class="sourceCode cell-code" id="cb234"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb234-1"><a href="#cb234-1" aria-hidden="true" tabindex="-1"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span><span class="op">*</span><span class="bu">len</span>(X))</span>
<span id="cb234-2"><a href="#cb234-2" aria-hidden="true" tabindex="-1"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(X))</span>
<span id="cb234-3"><a href="#cb234-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb234-4"><a href="#cb234-4" aria-hidden="true" tabindex="-1"></a>Xtr, Ytr <span class="op">=</span> X[:n1], Y[:n1]</span>
<span id="cb234-5"><a href="#cb234-5" aria-hidden="true" tabindex="-1"></a>Xdev, Ydev <span class="op">=</span> X[n1:n2], Y[n1:n2]</span>
<span id="cb234-6"><a href="#cb234-6" aria-hidden="true" tabindex="-1"></a>Xte, Yte <span class="op">=</span> X[n2:], Y[n2:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-173" class="cell">
<div class="sourceCode cell-code" id="cb235"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb235-1"><a href="#cb235-1" aria-hidden="true" tabindex="-1"></a>Xtr.shape, Xdev.shape, Xte.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([182516, 3]), torch.Size([22815, 3]), torch.Size([22815, 3]))</code></pre>
</div>
</div>
<div id="cell-174" class="cell">
<div class="sourceCode cell-code" id="cb237"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb237-1"><a href="#cb237-1" aria-hidden="true" tabindex="-1"></a>tracker <span class="op">=</span> {<span class="st">'lr'</span>:[], <span class="st">'batch_sz'</span>:[], <span class="st">'loss'</span>:[], <span class="st">'block_sz'</span>:[], <span class="st">'emb_sz'</span>:[], <span class="st">'hidden_units'</span>:[] }</span>
<span id="cb237-2"><a href="#cb237-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i))</span>
<span id="cb237-3"><a href="#cb237-3" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)<span class="op">;</span> F.cross_entropy(model(X), Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(16.8073, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-132-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-175" class="cell">
<div class="sourceCode cell-code" id="cb239"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb239-1"><a href="#cb239-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="dv">1</span>, epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(15.8100, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(5.0826, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(6.5589, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(7.4009, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(3.6053, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(4.3987, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(3.9632, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(4.5862, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(5.1175, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(4.5446, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(3.0278, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-176" class="cell">
<div class="sourceCode cell-code" id="cb242"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb242-1"><a href="#cb242-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="fl">0.001</span>, epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(3.0423, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(2.7571, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(3.2970, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(3.1602, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(3.0920, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(2.7909, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(3.0061, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(2.8765, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(3.0454, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(3.0402, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(2.7558, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-177" class="cell">
<div class="sourceCode cell-code" id="cb245"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb245-1"><a href="#cb245-1" aria-hidden="true" tabindex="-1"></a>tracker <span class="op">=</span> {<span class="st">'lr'</span>:[], <span class="st">'batch_sz'</span>:[], <span class="st">'loss'</span>:[], <span class="st">'block_sz'</span>:[], <span class="st">'emb_sz'</span>:[], <span class="st">'hidden_units'</span>:[] }</span>
<span id="cb245-2"><a href="#cb245-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i))</span>
<span id="cb245-3"><a href="#cb245-3" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)<span class="op">;</span> F.cross_entropy(model(X), Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(16.4552, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-135-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-178" class="cell">
<div class="sourceCode cell-code" id="cb247"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb247-1"><a href="#cb247-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span>lr_scheduler(steps<span class="op">=</span><span class="dv">1000</span>), epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(17.6297, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(16.3774, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(11.9241, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(9.8699, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(6.3115, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(4.4366, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(3.4436, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(3.6737, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(2.9998, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(4.9847, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(10.1631, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-179" class="cell">
<div class="sourceCode cell-code" id="cb250"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb250-1"><a href="#cb250-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(torch.tensor(tracker['lr']).log(), tracker['loss'])</span></span>
<span id="cb250-2"><a href="#cb250-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb250-3"><a href="#cb250-3" aria-hidden="true" tabindex="-1"></a>lrs <span class="op">=</span> torch.tensor(tracker[<span class="st">'lr'</span>])</span>
<span id="cb250-4"><a href="#cb250-4" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> torch.tensor(tracker[<span class="st">'loss'</span>])</span>
<span id="cb250-5"><a href="#cb250-5" aria-hidden="true" tabindex="-1"></a>log_lrs <span class="op">=</span> lrs.log()</span>
<span id="cb250-6"><a href="#cb250-6" aria-hidden="true" tabindex="-1"></a>min_idx <span class="op">=</span> losses.<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).indices.item()</span>
<span id="cb250-7"><a href="#cb250-7" aria-hidden="true" tabindex="-1"></a>plt.plot(log_lrs, tracker[<span class="st">'loss'</span>])</span>
<span id="cb250-8"><a href="#cb250-8" aria-hidden="true" tabindex="-1"></a>plt.axvline(log_lrs[min_idx], color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb250-9"><a href="#cb250-9" aria-hidden="true" tabindex="-1"></a>plt.text(log_lrs[min_idx]<span class="op">*</span><span class="fl">0.9</span>,losses[min_idx]<span class="op">*</span><span class="fl">0.9</span>, <span class="ss">f"lr : </span><span class="sc">{</span>lrs[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">, loss : </span><span class="sc">{</span>losses[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(tensor(-1.7114), tensor(2.3272), 'lr : 0.15, loss : 2.59')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-137-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-180" class="cell">
<div class="sourceCode cell-code" id="cb252"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb252-1"><a href="#cb252-1" aria-hidden="true" tabindex="-1"></a>plt.plot(tracker[<span class="st">'lr'</span>], tracker[<span class="st">'loss'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-138-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-181" class="cell">
<div class="sourceCode cell-code" id="cb253"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb253-1"><a href="#cb253-1" aria-hidden="true" tabindex="-1"></a>min_idx <span class="op">=</span> torch.tensor(tracker[<span class="st">'loss'</span>]).<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).indices.item()</span>
<span id="cb253-2"><a href="#cb253-2" aria-hidden="true" tabindex="-1"></a>tracker[<span class="st">'lr'</span>][min_idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.14933930337429047</code></pre>
</div>
</div>
<div id="cell-182" class="cell">
<div class="sourceCode cell-code" id="cb255"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb255-1"><a href="#cb255-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="fl">0.15</span>, epochs<span class="op">=</span><span class="dv">10000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(6.6876, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(3.4560, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(2.7864, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(2.5245, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(2.9045, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(2.5189, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(2.5428, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(2.5310, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(2.6385, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(2.2941, grad_fn=&lt;NllLossBackward0&gt;)
1000 tensor(2.5804, grad_fn=&lt;NllLossBackward0&gt;)
1100 tensor(2.3543, grad_fn=&lt;NllLossBackward0&gt;)
1200 tensor(2.4925, grad_fn=&lt;NllLossBackward0&gt;)
1300 tensor(2.0018, grad_fn=&lt;NllLossBackward0&gt;)
1400 tensor(1.8078, grad_fn=&lt;NllLossBackward0&gt;)
1500 tensor(2.3858, grad_fn=&lt;NllLossBackward0&gt;)
1600 tensor(2.2246, grad_fn=&lt;NllLossBackward0&gt;)
1700 tensor(2.6969, grad_fn=&lt;NllLossBackward0&gt;)
1800 tensor(2.6575, grad_fn=&lt;NllLossBackward0&gt;)
1900 tensor(2.5793, grad_fn=&lt;NllLossBackward0&gt;)
2000 tensor(2.3353, grad_fn=&lt;NllLossBackward0&gt;)
2100 tensor(2.4477, grad_fn=&lt;NllLossBackward0&gt;)
2200 tensor(2.5384, grad_fn=&lt;NllLossBackward0&gt;)
2300 tensor(2.5009, grad_fn=&lt;NllLossBackward0&gt;)
2400 tensor(2.5354, grad_fn=&lt;NllLossBackward0&gt;)
2500 tensor(2.5816, grad_fn=&lt;NllLossBackward0&gt;)
2600 tensor(2.4079, grad_fn=&lt;NllLossBackward0&gt;)
2700 tensor(2.6247, grad_fn=&lt;NllLossBackward0&gt;)
2800 tensor(2.3489, grad_fn=&lt;NllLossBackward0&gt;)
2900 tensor(3.1583, grad_fn=&lt;NllLossBackward0&gt;)
3000 tensor(2.5238, grad_fn=&lt;NllLossBackward0&gt;)
3100 tensor(2.4488, grad_fn=&lt;NllLossBackward0&gt;)
3200 tensor(2.5398, grad_fn=&lt;NllLossBackward0&gt;)
3300 tensor(2.8341, grad_fn=&lt;NllLossBackward0&gt;)
3400 tensor(2.4403, grad_fn=&lt;NllLossBackward0&gt;)
3500 tensor(2.6229, grad_fn=&lt;NllLossBackward0&gt;)
3600 tensor(3.0385, grad_fn=&lt;NllLossBackward0&gt;)
3700 tensor(2.1738, grad_fn=&lt;NllLossBackward0&gt;)
3800 tensor(2.1163, grad_fn=&lt;NllLossBackward0&gt;)
3900 tensor(2.0561, grad_fn=&lt;NllLossBackward0&gt;)
4000 tensor(2.3735, grad_fn=&lt;NllLossBackward0&gt;)
4100 tensor(2.5524, grad_fn=&lt;NllLossBackward0&gt;)
4200 tensor(2.2683, grad_fn=&lt;NllLossBackward0&gt;)
4300 tensor(2.7074, grad_fn=&lt;NllLossBackward0&gt;)
4400 tensor(2.3934, grad_fn=&lt;NllLossBackward0&gt;)
4500 tensor(2.4820, grad_fn=&lt;NllLossBackward0&gt;)
4600 tensor(2.2088, grad_fn=&lt;NllLossBackward0&gt;)
4700 tensor(2.8419, grad_fn=&lt;NllLossBackward0&gt;)
4800 tensor(2.2351, grad_fn=&lt;NllLossBackward0&gt;)
4900 tensor(2.4234, grad_fn=&lt;NllLossBackward0&gt;)
5000 tensor(3.1121, grad_fn=&lt;NllLossBackward0&gt;)
5100 tensor(2.6431, grad_fn=&lt;NllLossBackward0&gt;)
5200 tensor(1.9471, grad_fn=&lt;NllLossBackward0&gt;)
5300 tensor(2.5575, grad_fn=&lt;NllLossBackward0&gt;)
5400 tensor(2.1987, grad_fn=&lt;NllLossBackward0&gt;)
5500 tensor(2.4526, grad_fn=&lt;NllLossBackward0&gt;)
5600 tensor(2.6608, grad_fn=&lt;NllLossBackward0&gt;)
5700 tensor(2.6547, grad_fn=&lt;NllLossBackward0&gt;)
5800 tensor(2.5504, grad_fn=&lt;NllLossBackward0&gt;)
5900 tensor(2.6858, grad_fn=&lt;NllLossBackward0&gt;)
6000 tensor(2.1402, grad_fn=&lt;NllLossBackward0&gt;)
6100 tensor(2.0707, grad_fn=&lt;NllLossBackward0&gt;)
6200 tensor(2.1560, grad_fn=&lt;NllLossBackward0&gt;)
6300 tensor(2.1194, grad_fn=&lt;NllLossBackward0&gt;)
6400 tensor(2.1946, grad_fn=&lt;NllLossBackward0&gt;)
6500 tensor(2.2235, grad_fn=&lt;NllLossBackward0&gt;)
6600 tensor(2.5352, grad_fn=&lt;NllLossBackward0&gt;)
6700 tensor(2.3657, grad_fn=&lt;NllLossBackward0&gt;)
6800 tensor(2.4586, grad_fn=&lt;NllLossBackward0&gt;)
6900 tensor(2.8760, grad_fn=&lt;NllLossBackward0&gt;)
7000 tensor(2.6729, grad_fn=&lt;NllLossBackward0&gt;)
7100 tensor(2.4407, grad_fn=&lt;NllLossBackward0&gt;)
7200 tensor(2.5248, grad_fn=&lt;NllLossBackward0&gt;)
7300 tensor(2.6962, grad_fn=&lt;NllLossBackward0&gt;)
7400 tensor(2.5939, grad_fn=&lt;NllLossBackward0&gt;)
7500 tensor(1.9897, grad_fn=&lt;NllLossBackward0&gt;)
7600 tensor(2.3998, grad_fn=&lt;NllLossBackward0&gt;)
7700 tensor(2.6619, grad_fn=&lt;NllLossBackward0&gt;)
7800 tensor(2.2664, grad_fn=&lt;NllLossBackward0&gt;)
7900 tensor(2.5160, grad_fn=&lt;NllLossBackward0&gt;)
8000 tensor(2.2547, grad_fn=&lt;NllLossBackward0&gt;)
8100 tensor(2.2947, grad_fn=&lt;NllLossBackward0&gt;)
8200 tensor(2.5037, grad_fn=&lt;NllLossBackward0&gt;)
8300 tensor(2.5094, grad_fn=&lt;NllLossBackward0&gt;)
8400 tensor(2.7159, grad_fn=&lt;NllLossBackward0&gt;)
8500 tensor(2.2861, grad_fn=&lt;NllLossBackward0&gt;)
8600 tensor(2.7620, grad_fn=&lt;NllLossBackward0&gt;)
8700 tensor(2.6155, grad_fn=&lt;NllLossBackward0&gt;)
8800 tensor(2.4387, grad_fn=&lt;NllLossBackward0&gt;)
8900 tensor(2.2700, grad_fn=&lt;NllLossBackward0&gt;)
9000 tensor(2.4717, grad_fn=&lt;NllLossBackward0&gt;)
9100 tensor(2.3831, grad_fn=&lt;NllLossBackward0&gt;)
9200 tensor(2.1436, grad_fn=&lt;NllLossBackward0&gt;)
9300 tensor(2.4489, grad_fn=&lt;NllLossBackward0&gt;)
9400 tensor(2.2472, grad_fn=&lt;NllLossBackward0&gt;)
9500 tensor(2.4473, grad_fn=&lt;NllLossBackward0&gt;)
9600 tensor(2.2306, grad_fn=&lt;NllLossBackward0&gt;)
9700 tensor(2.3751, grad_fn=&lt;NllLossBackward0&gt;)
9800 tensor(2.1689, grad_fn=&lt;NllLossBackward0&gt;)
9900 tensor(2.3316, grad_fn=&lt;NllLossBackward0&gt;)
9999 tensor(2.4485, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-183" class="cell">
<div class="sourceCode cell-code" id="cb258"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb258-1"><a href="#cb258-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="fl">0.015</span>, epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(2.2572, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(2.9018, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(2.5827, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(2.1277, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(1.9535, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(2.6978, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(2.4004, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(2.1902, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(2.1744, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(2.3676, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(2.4979, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-184" class="cell">
<div class="sourceCode cell-code" id="cb261"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb261-1"><a href="#cb261-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="fl">0.015</span>, epochs<span class="op">=</span><span class="dv">100</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(2.4496, grad_fn=&lt;NllLossBackward0&gt;)
99 tensor(2.1163, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-185" class="cell">
<div class="sourceCode cell-code" id="cb264"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb264-1"><a href="#cb264-1" aria-hidden="true" tabindex="-1"></a>F.cross_entropy(model(Xdev), Ydev)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(2.5165, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<p>Some of these numbers indicate (training scores &lt; test scores). Our model is starting to overfit. Additionally model is not fully able to capture complexity and go down further as it’s a simpler model. Perhaps increasing the size of model might help</p>
</section>
<section id="increasing-number-of-neurons" class="level3">
<h3 class="anchored" data-anchor-id="increasing-number-of-neurons">Increasing number of neurons</h3>
<div id="cell-188" class="cell">
<div class="sourceCode cell-code" id="cb266"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb266-1"><a href="#cb266-1" aria-hidden="true" tabindex="-1"></a>tracker <span class="op">=</span> {<span class="st">'lr'</span>:[], <span class="st">'batch_sz'</span>:[], <span class="st">'loss'</span>:[], <span class="st">'block_sz'</span>:[], <span class="st">'emb_sz'</span>:[], <span class="st">'hidden_units'</span>:[] }</span>
<span id="cb266-2"><a href="#cb266-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i), hidden_units<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb266-3"><a href="#cb266-3" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)<span class="op">;</span> F.cross_entropy(model(X), Y) , model.num_params(), model.hidden_units</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-144-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-189" class="cell">
<div class="sourceCode cell-code" id="cb267"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb267-1"><a href="#cb267-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span>lr_scheduler(steps<span class="op">=</span><span class="dv">1000</span>), epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(40.0518, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(36.5316, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(24.7094, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(16.4738, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(7.5011, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(5.0571, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(12.2282, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(11.3392, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(29.6099, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(46.9556, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(89.7707, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-190" class="cell">
<div class="sourceCode cell-code" id="cb270"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb270-1"><a href="#cb270-1" aria-hidden="true" tabindex="-1"></a>lrs <span class="op">=</span> torch.tensor(tracker[<span class="st">'lr'</span>])</span>
<span id="cb270-2"><a href="#cb270-2" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> torch.tensor(tracker[<span class="st">'loss'</span>])</span>
<span id="cb270-3"><a href="#cb270-3" aria-hidden="true" tabindex="-1"></a>log_lrs <span class="op">=</span> lrs.log()</span>
<span id="cb270-4"><a href="#cb270-4" aria-hidden="true" tabindex="-1"></a>min_idx <span class="op">=</span> losses.<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).indices.item()</span>
<span id="cb270-5"><a href="#cb270-5" aria-hidden="true" tabindex="-1"></a>plt.plot(log_lrs, tracker[<span class="st">'loss'</span>])</span>
<span id="cb270-6"><a href="#cb270-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(log_lrs[min_idx], color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb270-7"><a href="#cb270-7" aria-hidden="true" tabindex="-1"></a>plt.text(log_lrs[min_idx]<span class="op">*</span><span class="fl">0.9</span>,losses[min_idx]<span class="op">*</span><span class="fl">0.9</span>, <span class="ss">f"lr : </span><span class="sc">{</span>lrs[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">, loss : </span><span class="sc">{</span>losses[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(tensor(-3.6032), tensor(2.5134), 'lr : 0.02, loss : 2.79')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-146-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-191" class="cell">
<div class="sourceCode cell-code" id="cb272"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb272-1"><a href="#cb272-1" aria-hidden="true" tabindex="-1"></a>plt.plot(tracker[<span class="st">'lr'</span>], tracker[<span class="st">'loss'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-147-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-192" class="cell">
<div class="sourceCode cell-code" id="cb273"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb273-1"><a href="#cb273-1" aria-hidden="true" tabindex="-1"></a>min_idx <span class="op">=</span> torch.tensor(tracker[<span class="st">'loss'</span>]).<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).indices.item()</span>
<span id="cb273-2"><a href="#cb273-2" aria-hidden="true" tabindex="-1"></a>tracker[<span class="st">'lr'</span>][min_idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.018249936401844025</code></pre>
</div>
</div>
<div id="cell-193" class="cell">
<div class="sourceCode cell-code" id="cb275"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb275-1"><a href="#cb275-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="fl">0.02</span>, epochs<span class="op">=</span><span class="dv">10000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(61.7644, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(21.7138, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(19.0193, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(18.1190, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(7.9473, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(9.0366, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(12.0193, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(8.7849, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(6.9708, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(7.7025, grad_fn=&lt;NllLossBackward0&gt;)
1000 tensor(8.2050, grad_fn=&lt;NllLossBackward0&gt;)
1100 tensor(8.4025, grad_fn=&lt;NllLossBackward0&gt;)
1200 tensor(9.7596, grad_fn=&lt;NllLossBackward0&gt;)
1300 tensor(8.0802, grad_fn=&lt;NllLossBackward0&gt;)
1400 tensor(8.7060, grad_fn=&lt;NllLossBackward0&gt;)
1500 tensor(8.4114, grad_fn=&lt;NllLossBackward0&gt;)
1600 tensor(7.5827, grad_fn=&lt;NllLossBackward0&gt;)
1700 tensor(4.9879, grad_fn=&lt;NllLossBackward0&gt;)
1800 tensor(7.0894, grad_fn=&lt;NllLossBackward0&gt;)
1900 tensor(6.0056, grad_fn=&lt;NllLossBackward0&gt;)
2000 tensor(8.0175, grad_fn=&lt;NllLossBackward0&gt;)
2100 tensor(6.4426, grad_fn=&lt;NllLossBackward0&gt;)
2200 tensor(3.8171, grad_fn=&lt;NllLossBackward0&gt;)
2300 tensor(7.5533, grad_fn=&lt;NllLossBackward0&gt;)
2400 tensor(5.0896, grad_fn=&lt;NllLossBackward0&gt;)
2500 tensor(4.9172, grad_fn=&lt;NllLossBackward0&gt;)
2600 tensor(7.5949, grad_fn=&lt;NllLossBackward0&gt;)
2700 tensor(6.5752, grad_fn=&lt;NllLossBackward0&gt;)
2800 tensor(6.7322, grad_fn=&lt;NllLossBackward0&gt;)
2900 tensor(4.4214, grad_fn=&lt;NllLossBackward0&gt;)
3000 tensor(5.9283, grad_fn=&lt;NllLossBackward0&gt;)
3100 tensor(5.0158, grad_fn=&lt;NllLossBackward0&gt;)
3200 tensor(8.4569, grad_fn=&lt;NllLossBackward0&gt;)
3300 tensor(4.9871, grad_fn=&lt;NllLossBackward0&gt;)
3400 tensor(5.5735, grad_fn=&lt;NllLossBackward0&gt;)
3500 tensor(5.1228, grad_fn=&lt;NllLossBackward0&gt;)
3600 tensor(3.9927, grad_fn=&lt;NllLossBackward0&gt;)
3700 tensor(5.9232, grad_fn=&lt;NllLossBackward0&gt;)
3800 tensor(6.7801, grad_fn=&lt;NllLossBackward0&gt;)
3900 tensor(6.8666, grad_fn=&lt;NllLossBackward0&gt;)
4000 tensor(4.8405, grad_fn=&lt;NllLossBackward0&gt;)
4100 tensor(6.0023, grad_fn=&lt;NllLossBackward0&gt;)
4200 tensor(4.4253, grad_fn=&lt;NllLossBackward0&gt;)
4300 tensor(5.2597, grad_fn=&lt;NllLossBackward0&gt;)
4400 tensor(4.2080, grad_fn=&lt;NllLossBackward0&gt;)
4500 tensor(6.0687, grad_fn=&lt;NllLossBackward0&gt;)
4600 tensor(3.6387, grad_fn=&lt;NllLossBackward0&gt;)
4700 tensor(6.1733, grad_fn=&lt;NllLossBackward0&gt;)
4800 tensor(4.8147, grad_fn=&lt;NllLossBackward0&gt;)
4900 tensor(3.5229, grad_fn=&lt;NllLossBackward0&gt;)
5000 tensor(3.9053, grad_fn=&lt;NllLossBackward0&gt;)
5100 tensor(5.3193, grad_fn=&lt;NllLossBackward0&gt;)
5200 tensor(4.8308, grad_fn=&lt;NllLossBackward0&gt;)
5300 tensor(4.0796, grad_fn=&lt;NllLossBackward0&gt;)
5400 tensor(5.7834, grad_fn=&lt;NllLossBackward0&gt;)
5500 tensor(4.8146, grad_fn=&lt;NllLossBackward0&gt;)
5600 tensor(5.5315, grad_fn=&lt;NllLossBackward0&gt;)
5700 tensor(3.8576, grad_fn=&lt;NllLossBackward0&gt;)
5800 tensor(4.4375, grad_fn=&lt;NllLossBackward0&gt;)
5900 tensor(3.5489, grad_fn=&lt;NllLossBackward0&gt;)
6000 tensor(3.8797, grad_fn=&lt;NllLossBackward0&gt;)
6100 tensor(3.6326, grad_fn=&lt;NllLossBackward0&gt;)
6200 tensor(3.4096, grad_fn=&lt;NllLossBackward0&gt;)
6300 tensor(3.2450, grad_fn=&lt;NllLossBackward0&gt;)
6400 tensor(4.4493, grad_fn=&lt;NllLossBackward0&gt;)
6500 tensor(3.8375, grad_fn=&lt;NllLossBackward0&gt;)
6600 tensor(5.1432, grad_fn=&lt;NllLossBackward0&gt;)
6700 tensor(5.3734, grad_fn=&lt;NllLossBackward0&gt;)
6800 tensor(3.5342, grad_fn=&lt;NllLossBackward0&gt;)
6900 tensor(3.8200, grad_fn=&lt;NllLossBackward0&gt;)
7000 tensor(2.4815, grad_fn=&lt;NllLossBackward0&gt;)
7100 tensor(4.2042, grad_fn=&lt;NllLossBackward0&gt;)
7200 tensor(3.9543, grad_fn=&lt;NllLossBackward0&gt;)
7300 tensor(5.9323, grad_fn=&lt;NllLossBackward0&gt;)
7400 tensor(3.1850, grad_fn=&lt;NllLossBackward0&gt;)
7500 tensor(3.9603, grad_fn=&lt;NllLossBackward0&gt;)
7600 tensor(3.3168, grad_fn=&lt;NllLossBackward0&gt;)
7700 tensor(5.2525, grad_fn=&lt;NllLossBackward0&gt;)
7800 tensor(3.5372, grad_fn=&lt;NllLossBackward0&gt;)
7900 tensor(3.5608, grad_fn=&lt;NllLossBackward0&gt;)
8000 tensor(3.2310, grad_fn=&lt;NllLossBackward0&gt;)
8100 tensor(5.8350, grad_fn=&lt;NllLossBackward0&gt;)
8200 tensor(3.8551, grad_fn=&lt;NllLossBackward0&gt;)
8300 tensor(4.7595, grad_fn=&lt;NllLossBackward0&gt;)
8400 tensor(4.8322, grad_fn=&lt;NllLossBackward0&gt;)
8500 tensor(3.0964, grad_fn=&lt;NllLossBackward0&gt;)
8600 tensor(3.3326, grad_fn=&lt;NllLossBackward0&gt;)
8700 tensor(5.5303, grad_fn=&lt;NllLossBackward0&gt;)
8800 tensor(4.6164, grad_fn=&lt;NllLossBackward0&gt;)
8900 tensor(4.6254, grad_fn=&lt;NllLossBackward0&gt;)
9000 tensor(2.9118, grad_fn=&lt;NllLossBackward0&gt;)
9100 tensor(3.8682, grad_fn=&lt;NllLossBackward0&gt;)
9200 tensor(6.0656, grad_fn=&lt;NllLossBackward0&gt;)
9300 tensor(4.2411, grad_fn=&lt;NllLossBackward0&gt;)
9400 tensor(4.2726, grad_fn=&lt;NllLossBackward0&gt;)
9500 tensor(4.1724, grad_fn=&lt;NllLossBackward0&gt;)
9600 tensor(4.7331, grad_fn=&lt;NllLossBackward0&gt;)
9700 tensor(3.8303, grad_fn=&lt;NllLossBackward0&gt;)
9800 tensor(3.4319, grad_fn=&lt;NllLossBackward0&gt;)
9900 tensor(3.7812, grad_fn=&lt;NllLossBackward0&gt;)
9999 tensor(4.8933, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-194" class="cell">
<div class="sourceCode cell-code" id="cb278"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb278-1"><a href="#cb278-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="fl">0.001</span>, epochs<span class="op">=</span><span class="dv">100</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker) <span class="co"># tried 0.02, 0.01, 0.001</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(4.8102, grad_fn=&lt;NllLossBackward0&gt;)
99 tensor(2.1143, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-195" class="cell">
<div class="sourceCode cell-code" id="cb281"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb281-1"><a href="#cb281-1" aria-hidden="true" tabindex="-1"></a>F.cross_entropy(model(Xdev), Ydev)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(4.7569, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-196" class="cell">
<div class="sourceCode cell-code" id="cb283"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb283-1"><a href="#cb283-1" aria-hidden="true" tabindex="-1"></a>model.num_params()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>34081</code></pre>
</div>
</div>
<div id="cell-197" class="cell">
<div class="sourceCode cell-code" id="cb285"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb285-1"><a href="#cb285-1" aria-hidden="true" tabindex="-1"></a>tracker <span class="op">=</span> {<span class="st">'lr'</span>:[], <span class="st">'batch_sz'</span>:[], <span class="st">'loss'</span>:[], <span class="st">'block_sz'</span>:[], <span class="st">'emb_sz'</span>:[], <span class="st">'hidden_units'</span>:[] }</span>
<span id="cb285-2"><a href="#cb285-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i), hidden_units<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb285-3"><a href="#cb285-3" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i)<span class="op">;</span> F.cross_entropy(model(X), Y) , model.num_params(), model.hidden_units</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-153-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-198" class="cell">
<div class="sourceCode cell-code" id="cb286"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb286-1"><a href="#cb286-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span>lr_scheduler(steps<span class="op">=</span><span class="dv">1000</span>), epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(37.3450, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(24.5692, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(15.8155, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(9.1198, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(4.8730, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(5.8543, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(5.2846, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(5.8586, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(9.6759, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(14.6072, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(18.3681, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-199" class="cell">
<div class="sourceCode cell-code" id="cb289"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb289-1"><a href="#cb289-1" aria-hidden="true" tabindex="-1"></a>lrs <span class="op">=</span> torch.tensor(tracker[<span class="st">'lr'</span>])</span>
<span id="cb289-2"><a href="#cb289-2" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> torch.tensor(tracker[<span class="st">'loss'</span>])</span>
<span id="cb289-3"><a href="#cb289-3" aria-hidden="true" tabindex="-1"></a>log_lrs <span class="op">=</span> lrs.log()</span>
<span id="cb289-4"><a href="#cb289-4" aria-hidden="true" tabindex="-1"></a>min_idx <span class="op">=</span> losses.<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).indices.item()</span>
<span id="cb289-5"><a href="#cb289-5" aria-hidden="true" tabindex="-1"></a>plt.plot(log_lrs, tracker[<span class="st">'loss'</span>])</span>
<span id="cb289-6"><a href="#cb289-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(log_lrs[min_idx], color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb289-7"><a href="#cb289-7" aria-hidden="true" tabindex="-1"></a>plt.text(log_lrs[min_idx]<span class="op">*</span><span class="fl">0.9</span>,losses[min_idx]<span class="op">*</span><span class="fl">0.9</span>, <span class="ss">f"lr : </span><span class="sc">{</span>lrs[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">, loss : </span><span class="sc">{</span>losses[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(tensor(-3.3294), tensor(2.5136), 'lr : 0.02, loss : 2.79')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-155-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-200" class="cell">
<div class="sourceCode cell-code" id="cb291"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb291-1"><a href="#cb291-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="fl">0.02</span>, epochs<span class="op">=</span><span class="dv">10000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(2.3650, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(2.5750, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(2.2345, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(2.6657, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(2.2304, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(3.2544, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(2.5204, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(2.7564, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(2.4260, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(2.9606, grad_fn=&lt;NllLossBackward0&gt;)
1000 tensor(2.8647, grad_fn=&lt;NllLossBackward0&gt;)
1100 tensor(2.7729, grad_fn=&lt;NllLossBackward0&gt;)
1200 tensor(2.2639, grad_fn=&lt;NllLossBackward0&gt;)
1300 tensor(2.4464, grad_fn=&lt;NllLossBackward0&gt;)
1400 tensor(2.3443, grad_fn=&lt;NllLossBackward0&gt;)
1500 tensor(2.8512, grad_fn=&lt;NllLossBackward0&gt;)
1600 tensor(2.6609, grad_fn=&lt;NllLossBackward0&gt;)
1700 tensor(2.9302, grad_fn=&lt;NllLossBackward0&gt;)
1800 tensor(2.7442, grad_fn=&lt;NllLossBackward0&gt;)
1900 tensor(3.3001, grad_fn=&lt;NllLossBackward0&gt;)
2000 tensor(2.3761, grad_fn=&lt;NllLossBackward0&gt;)
2100 tensor(2.6010, grad_fn=&lt;NllLossBackward0&gt;)
2200 tensor(2.3317, grad_fn=&lt;NllLossBackward0&gt;)
2300 tensor(2.3662, grad_fn=&lt;NllLossBackward0&gt;)
2400 tensor(2.9355, grad_fn=&lt;NllLossBackward0&gt;)
2500 tensor(2.5726, grad_fn=&lt;NllLossBackward0&gt;)
2600 tensor(2.1554, grad_fn=&lt;NllLossBackward0&gt;)
2700 tensor(2.4191, grad_fn=&lt;NllLossBackward0&gt;)
2800 tensor(2.4981, grad_fn=&lt;NllLossBackward0&gt;)
2900 tensor(2.4923, grad_fn=&lt;NllLossBackward0&gt;)
3000 tensor(2.1398, grad_fn=&lt;NllLossBackward0&gt;)
3100 tensor(2.4383, grad_fn=&lt;NllLossBackward0&gt;)
3200 tensor(2.0546, grad_fn=&lt;NllLossBackward0&gt;)
3300 tensor(2.2128, grad_fn=&lt;NllLossBackward0&gt;)
3400 tensor(2.0385, grad_fn=&lt;NllLossBackward0&gt;)
3500 tensor(2.3519, grad_fn=&lt;NllLossBackward0&gt;)
3600 tensor(2.5346, grad_fn=&lt;NllLossBackward0&gt;)
3700 tensor(2.3406, grad_fn=&lt;NllLossBackward0&gt;)
3800 tensor(2.7333, grad_fn=&lt;NllLossBackward0&gt;)
3900 tensor(2.5097, grad_fn=&lt;NllLossBackward0&gt;)
4000 tensor(2.6590, grad_fn=&lt;NllLossBackward0&gt;)
4100 tensor(2.4219, grad_fn=&lt;NllLossBackward0&gt;)
4200 tensor(2.2782, grad_fn=&lt;NllLossBackward0&gt;)
4300 tensor(2.1400, grad_fn=&lt;NllLossBackward0&gt;)
4400 tensor(2.7256, grad_fn=&lt;NllLossBackward0&gt;)
4500 tensor(2.4548, grad_fn=&lt;NllLossBackward0&gt;)
4600 tensor(2.6635, grad_fn=&lt;NllLossBackward0&gt;)
4700 tensor(2.3617, grad_fn=&lt;NllLossBackward0&gt;)
4800 tensor(2.1300, grad_fn=&lt;NllLossBackward0&gt;)
4900 tensor(2.2647, grad_fn=&lt;NllLossBackward0&gt;)
5000 tensor(2.3157, grad_fn=&lt;NllLossBackward0&gt;)
5100 tensor(2.7554, grad_fn=&lt;NllLossBackward0&gt;)
5200 tensor(2.5484, grad_fn=&lt;NllLossBackward0&gt;)
5300 tensor(2.7912, grad_fn=&lt;NllLossBackward0&gt;)
5400 tensor(2.6207, grad_fn=&lt;NllLossBackward0&gt;)
5500 tensor(2.2788, grad_fn=&lt;NllLossBackward0&gt;)
5600 tensor(2.2430, grad_fn=&lt;NllLossBackward0&gt;)
5700 tensor(2.7456, grad_fn=&lt;NllLossBackward0&gt;)
5800 tensor(2.6865, grad_fn=&lt;NllLossBackward0&gt;)
5900 tensor(2.3350, grad_fn=&lt;NllLossBackward0&gt;)
6000 tensor(2.8199, grad_fn=&lt;NllLossBackward0&gt;)
6100 tensor(2.2868, grad_fn=&lt;NllLossBackward0&gt;)
6200 tensor(2.5639, grad_fn=&lt;NllLossBackward0&gt;)
6300 tensor(2.7103, grad_fn=&lt;NllLossBackward0&gt;)
6400 tensor(2.3779, grad_fn=&lt;NllLossBackward0&gt;)
6500 tensor(2.5291, grad_fn=&lt;NllLossBackward0&gt;)
6600 tensor(2.6559, grad_fn=&lt;NllLossBackward0&gt;)
6700 tensor(2.6285, grad_fn=&lt;NllLossBackward0&gt;)
6800 tensor(2.3053, grad_fn=&lt;NllLossBackward0&gt;)
6900 tensor(2.8059, grad_fn=&lt;NllLossBackward0&gt;)
7000 tensor(3.0762, grad_fn=&lt;NllLossBackward0&gt;)
7100 tensor(2.6888, grad_fn=&lt;NllLossBackward0&gt;)
7200 tensor(2.3106, grad_fn=&lt;NllLossBackward0&gt;)
7300 tensor(2.4496, grad_fn=&lt;NllLossBackward0&gt;)
7400 tensor(3.3963, grad_fn=&lt;NllLossBackward0&gt;)
7500 tensor(2.5796, grad_fn=&lt;NllLossBackward0&gt;)
7600 tensor(2.2029, grad_fn=&lt;NllLossBackward0&gt;)
7700 tensor(2.6832, grad_fn=&lt;NllLossBackward0&gt;)
7800 tensor(2.5828, grad_fn=&lt;NllLossBackward0&gt;)
7900 tensor(2.4692, grad_fn=&lt;NllLossBackward0&gt;)
8000 tensor(2.3280, grad_fn=&lt;NllLossBackward0&gt;)
8100 tensor(2.6653, grad_fn=&lt;NllLossBackward0&gt;)
8200 tensor(2.4018, grad_fn=&lt;NllLossBackward0&gt;)
8300 tensor(2.2335, grad_fn=&lt;NllLossBackward0&gt;)
8400 tensor(2.6129, grad_fn=&lt;NllLossBackward0&gt;)
8500 tensor(2.4332, grad_fn=&lt;NllLossBackward0&gt;)
8600 tensor(2.2469, grad_fn=&lt;NllLossBackward0&gt;)
8700 tensor(2.7958, grad_fn=&lt;NllLossBackward0&gt;)
8800 tensor(2.9870, grad_fn=&lt;NllLossBackward0&gt;)
8900 tensor(2.7271, grad_fn=&lt;NllLossBackward0&gt;)
9000 tensor(2.9479, grad_fn=&lt;NllLossBackward0&gt;)
9100 tensor(2.2839, grad_fn=&lt;NllLossBackward0&gt;)
9200 tensor(2.4650, grad_fn=&lt;NllLossBackward0&gt;)
9300 tensor(2.4145, grad_fn=&lt;NllLossBackward0&gt;)
9400 tensor(2.3716, grad_fn=&lt;NllLossBackward0&gt;)
9500 tensor(2.5362, grad_fn=&lt;NllLossBackward0&gt;)
9600 tensor(2.3769, grad_fn=&lt;NllLossBackward0&gt;)
9700 tensor(2.5510, grad_fn=&lt;NllLossBackward0&gt;)
9800 tensor(2.5588, grad_fn=&lt;NllLossBackward0&gt;)
9900 tensor(2.4834, grad_fn=&lt;NllLossBackward0&gt;)
9999 tensor(2.4656, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-201" class="cell">
<div class="sourceCode cell-code" id="cb294"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb294-1"><a href="#cb294-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="fl">0.002</span>, epochs<span class="op">=</span><span class="dv">10000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(2.5914, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(2.6015, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(2.6615, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(2.1866, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(2.4424, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(2.3740, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(2.2900, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(2.4071, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(2.3903, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(2.4372, grad_fn=&lt;NllLossBackward0&gt;)
1000 tensor(2.7021, grad_fn=&lt;NllLossBackward0&gt;)
1100 tensor(2.3557, grad_fn=&lt;NllLossBackward0&gt;)
1200 tensor(2.4595, grad_fn=&lt;NllLossBackward0&gt;)
1300 tensor(2.2065, grad_fn=&lt;NllLossBackward0&gt;)
1400 tensor(2.4413, grad_fn=&lt;NllLossBackward0&gt;)
1500 tensor(2.3990, grad_fn=&lt;NllLossBackward0&gt;)
1600 tensor(2.3373, grad_fn=&lt;NllLossBackward0&gt;)
1700 tensor(2.6296, grad_fn=&lt;NllLossBackward0&gt;)
1800 tensor(2.5524, grad_fn=&lt;NllLossBackward0&gt;)
1900 tensor(2.4687, grad_fn=&lt;NllLossBackward0&gt;)
2000 tensor(2.2817, grad_fn=&lt;NllLossBackward0&gt;)
2100 tensor(2.3427, grad_fn=&lt;NllLossBackward0&gt;)
2200 tensor(2.3225, grad_fn=&lt;NllLossBackward0&gt;)
2300 tensor(2.3668, grad_fn=&lt;NllLossBackward0&gt;)
2400 tensor(2.3957, grad_fn=&lt;NllLossBackward0&gt;)
2500 tensor(2.4984, grad_fn=&lt;NllLossBackward0&gt;)
2600 tensor(2.4131, grad_fn=&lt;NllLossBackward0&gt;)
2700 tensor(2.1443, grad_fn=&lt;NllLossBackward0&gt;)
2800 tensor(2.4484, grad_fn=&lt;NllLossBackward0&gt;)
2900 tensor(2.5513, grad_fn=&lt;NllLossBackward0&gt;)
3000 tensor(2.1167, grad_fn=&lt;NllLossBackward0&gt;)
3100 tensor(2.5792, grad_fn=&lt;NllLossBackward0&gt;)
3200 tensor(2.3018, grad_fn=&lt;NllLossBackward0&gt;)
3300 tensor(2.2011, grad_fn=&lt;NllLossBackward0&gt;)
3400 tensor(3.1988, grad_fn=&lt;NllLossBackward0&gt;)
3500 tensor(2.6448, grad_fn=&lt;NllLossBackward0&gt;)
3600 tensor(2.6687, grad_fn=&lt;NllLossBackward0&gt;)
3700 tensor(2.2027, grad_fn=&lt;NllLossBackward0&gt;)
3800 tensor(2.6672, grad_fn=&lt;NllLossBackward0&gt;)
3900 tensor(2.3656, grad_fn=&lt;NllLossBackward0&gt;)
4000 tensor(2.3074, grad_fn=&lt;NllLossBackward0&gt;)
4100 tensor(2.5742, grad_fn=&lt;NllLossBackward0&gt;)
4200 tensor(2.4304, grad_fn=&lt;NllLossBackward0&gt;)
4300 tensor(2.4862, grad_fn=&lt;NllLossBackward0&gt;)
4400 tensor(2.3051, grad_fn=&lt;NllLossBackward0&gt;)
4500 tensor(2.6738, grad_fn=&lt;NllLossBackward0&gt;)
4600 tensor(2.7503, grad_fn=&lt;NllLossBackward0&gt;)
4700 tensor(2.6009, grad_fn=&lt;NllLossBackward0&gt;)
4800 tensor(2.5119, grad_fn=&lt;NllLossBackward0&gt;)
4900 tensor(2.2779, grad_fn=&lt;NllLossBackward0&gt;)
5000 tensor(2.4389, grad_fn=&lt;NllLossBackward0&gt;)
5100 tensor(2.1812, grad_fn=&lt;NllLossBackward0&gt;)
5200 tensor(2.6321, grad_fn=&lt;NllLossBackward0&gt;)
5300 tensor(2.2712, grad_fn=&lt;NllLossBackward0&gt;)
5400 tensor(2.3459, grad_fn=&lt;NllLossBackward0&gt;)
5500 tensor(2.3148, grad_fn=&lt;NllLossBackward0&gt;)
5600 tensor(2.5690, grad_fn=&lt;NllLossBackward0&gt;)
5700 tensor(2.1299, grad_fn=&lt;NllLossBackward0&gt;)
5800 tensor(2.6001, grad_fn=&lt;NllLossBackward0&gt;)
5900 tensor(2.0995, grad_fn=&lt;NllLossBackward0&gt;)
6000 tensor(2.9337, grad_fn=&lt;NllLossBackward0&gt;)
6100 tensor(2.2322, grad_fn=&lt;NllLossBackward0&gt;)
6200 tensor(2.6015, grad_fn=&lt;NllLossBackward0&gt;)
6300 tensor(2.2005, grad_fn=&lt;NllLossBackward0&gt;)
6400 tensor(2.7809, grad_fn=&lt;NllLossBackward0&gt;)
6500 tensor(2.0445, grad_fn=&lt;NllLossBackward0&gt;)
6600 tensor(2.4713, grad_fn=&lt;NllLossBackward0&gt;)
6700 tensor(2.0280, grad_fn=&lt;NllLossBackward0&gt;)
6800 tensor(2.1322, grad_fn=&lt;NllLossBackward0&gt;)
6900 tensor(2.4984, grad_fn=&lt;NllLossBackward0&gt;)
7000 tensor(2.5776, grad_fn=&lt;NllLossBackward0&gt;)
7100 tensor(2.3631, grad_fn=&lt;NllLossBackward0&gt;)
7200 tensor(2.0526, grad_fn=&lt;NllLossBackward0&gt;)
7300 tensor(2.1573, grad_fn=&lt;NllLossBackward0&gt;)
7400 tensor(2.3049, grad_fn=&lt;NllLossBackward0&gt;)
7500 tensor(2.6727, grad_fn=&lt;NllLossBackward0&gt;)
7600 tensor(2.3522, grad_fn=&lt;NllLossBackward0&gt;)
7700 tensor(2.1579, grad_fn=&lt;NllLossBackward0&gt;)
7800 tensor(2.4094, grad_fn=&lt;NllLossBackward0&gt;)
7900 tensor(2.4582, grad_fn=&lt;NllLossBackward0&gt;)
8000 tensor(2.4877, grad_fn=&lt;NllLossBackward0&gt;)
8100 tensor(2.2700, grad_fn=&lt;NllLossBackward0&gt;)
8200 tensor(2.5825, grad_fn=&lt;NllLossBackward0&gt;)
8300 tensor(3.0091, grad_fn=&lt;NllLossBackward0&gt;)
8400 tensor(2.4350, grad_fn=&lt;NllLossBackward0&gt;)
8500 tensor(1.9669, grad_fn=&lt;NllLossBackward0&gt;)
8600 tensor(2.6937, grad_fn=&lt;NllLossBackward0&gt;)
8700 tensor(2.7113, grad_fn=&lt;NllLossBackward0&gt;)
8800 tensor(2.0843, grad_fn=&lt;NllLossBackward0&gt;)
8900 tensor(2.2009, grad_fn=&lt;NllLossBackward0&gt;)
9000 tensor(2.3926, grad_fn=&lt;NllLossBackward0&gt;)
9100 tensor(2.3269, grad_fn=&lt;NllLossBackward0&gt;)
9200 tensor(2.1644, grad_fn=&lt;NllLossBackward0&gt;)
9300 tensor(2.5310, grad_fn=&lt;NllLossBackward0&gt;)
9400 tensor(2.2162, grad_fn=&lt;NllLossBackward0&gt;)
9500 tensor(2.3658, grad_fn=&lt;NllLossBackward0&gt;)
9600 tensor(2.5182, grad_fn=&lt;NllLossBackward0&gt;)
9700 tensor(2.5066, grad_fn=&lt;NllLossBackward0&gt;)
9800 tensor(1.9810, grad_fn=&lt;NllLossBackward0&gt;)
9900 tensor(2.5331, grad_fn=&lt;NllLossBackward0&gt;)
9999 tensor(2.2161, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-202" class="cell">
<div class="sourceCode cell-code" id="cb297"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb297-1"><a href="#cb297-1" aria-hidden="true" tabindex="-1"></a>F.cross_entropy(model(Xdev), Ydev)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(2.6687, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="increasing-embedding-size" class="level3">
<h3 class="anchored" data-anchor-id="increasing-embedding-size">Increasing Embedding size</h3>
<ul>
<li>We might want to keep parameters close to ~10K</li>
</ul>
<div id="cell-204" class="cell">
<div class="sourceCode cell-code" id="cb299"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb299-1"><a href="#cb299-1" aria-hidden="true" tabindex="-1"></a>tracker <span class="op">=</span> {<span class="st">'lr'</span>:[], <span class="st">'batch_sz'</span>:[], <span class="st">'loss'</span>:[], <span class="st">'block_sz'</span>:[], <span class="st">'emb_sz'</span>:[], <span class="st">'hidden_units'</span>:[] }</span>
<span id="cb299-2"><a href="#cb299-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(<span class="bu">len</span>(s2i), hidden_units<span class="op">=</span><span class="dv">200</span>, emb_sz<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb299-3"><a href="#cb299-3" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i, emb_model <span class="op">=</span> TSNE(random_state<span class="op">=</span><span class="dv">0</span>, n_iter<span class="op">=</span><span class="dv">1000</span>, perplexity<span class="op">=</span><span class="dv">26</span>), cluster_model<span class="op">=</span>KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>) )<span class="op">;</span> F.cross_entropy(model(X), Y) , model.num_params(), model.hidden_units</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/homebrew/Caskroom/miniforge/base/envs/aiking/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-159-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-205" class="cell">
<div class="sourceCode cell-code" id="cb301"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb301-1"><a href="#cb301-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span>lr_scheduler(steps<span class="op">=</span><span class="dv">1000</span>), epochs<span class="op">=</span><span class="dv">1000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(23.6677, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(15.7213, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(17.7044, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(15.3154, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(11.2020, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(13.0558, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(6.7073, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(7.7102, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(10.5667, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(10.3973, grad_fn=&lt;NllLossBackward0&gt;)
999 tensor(14.0128, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-206" class="cell">
<div class="sourceCode cell-code" id="cb304"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb304-1"><a href="#cb304-1" aria-hidden="true" tabindex="-1"></a>lrs <span class="op">=</span> torch.tensor(tracker[<span class="st">'lr'</span>])</span>
<span id="cb304-2"><a href="#cb304-2" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> torch.tensor(tracker[<span class="st">'loss'</span>])</span>
<span id="cb304-3"><a href="#cb304-3" aria-hidden="true" tabindex="-1"></a>log_lrs <span class="op">=</span> lrs.log()</span>
<span id="cb304-4"><a href="#cb304-4" aria-hidden="true" tabindex="-1"></a>min_idx <span class="op">=</span> losses.<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).indices.item()</span>
<span id="cb304-5"><a href="#cb304-5" aria-hidden="true" tabindex="-1"></a>plt.plot(log_lrs, tracker[<span class="st">'loss'</span>])</span>
<span id="cb304-6"><a href="#cb304-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(log_lrs[min_idx], color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb304-7"><a href="#cb304-7" aria-hidden="true" tabindex="-1"></a>plt.text(log_lrs[min_idx]<span class="op">*</span><span class="fl">0.9</span>,losses[min_idx]<span class="op">*</span><span class="fl">0.9</span>, <span class="ss">f"lr : </span><span class="sc">{</span>lrs[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">, loss : </span><span class="sc">{</span>losses[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(tensor(-2.0163), tensor(3.5029), 'lr : 0.11, loss : 3.89')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-161-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-207" class="cell">
<div class="sourceCode cell-code" id="cb306"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb306-1"><a href="#cb306-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="fl">0.11</span>, epochs<span class="op">=</span><span class="dv">10000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(16.1025, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(4.2531, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(3.6509, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(5.5972, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(3.5624, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(3.7660, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(4.1625, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(3.8562, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(2.3530, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(3.7563, grad_fn=&lt;NllLossBackward0&gt;)
1000 tensor(4.1694, grad_fn=&lt;NllLossBackward0&gt;)
1100 tensor(3.5898, grad_fn=&lt;NllLossBackward0&gt;)
1200 tensor(3.3913, grad_fn=&lt;NllLossBackward0&gt;)
1300 tensor(3.1442, grad_fn=&lt;NllLossBackward0&gt;)
1400 tensor(2.4846, grad_fn=&lt;NllLossBackward0&gt;)
1500 tensor(3.7013, grad_fn=&lt;NllLossBackward0&gt;)
1600 tensor(3.6672, grad_fn=&lt;NllLossBackward0&gt;)
1700 tensor(3.1273, grad_fn=&lt;NllLossBackward0&gt;)
1800 tensor(2.4491, grad_fn=&lt;NllLossBackward0&gt;)
1900 tensor(3.5135, grad_fn=&lt;NllLossBackward0&gt;)
2000 tensor(3.2433, grad_fn=&lt;NllLossBackward0&gt;)
2100 tensor(3.0996, grad_fn=&lt;NllLossBackward0&gt;)
2200 tensor(2.4086, grad_fn=&lt;NllLossBackward0&gt;)
2300 tensor(2.5187, grad_fn=&lt;NllLossBackward0&gt;)
2400 tensor(2.5878, grad_fn=&lt;NllLossBackward0&gt;)
2500 tensor(2.5127, grad_fn=&lt;NllLossBackward0&gt;)
2600 tensor(2.6652, grad_fn=&lt;NllLossBackward0&gt;)
2700 tensor(2.3650, grad_fn=&lt;NllLossBackward0&gt;)
2800 tensor(3.1310, grad_fn=&lt;NllLossBackward0&gt;)
2900 tensor(3.1550, grad_fn=&lt;NllLossBackward0&gt;)
3000 tensor(3.3197, grad_fn=&lt;NllLossBackward0&gt;)
3100 tensor(2.4888, grad_fn=&lt;NllLossBackward0&gt;)
3200 tensor(3.0126, grad_fn=&lt;NllLossBackward0&gt;)
3300 tensor(3.4476, grad_fn=&lt;NllLossBackward0&gt;)
3400 tensor(2.4726, grad_fn=&lt;NllLossBackward0&gt;)
3500 tensor(2.4124, grad_fn=&lt;NllLossBackward0&gt;)
3600 tensor(2.2420, grad_fn=&lt;NllLossBackward0&gt;)
3700 tensor(2.1160, grad_fn=&lt;NllLossBackward0&gt;)
3800 tensor(2.7342, grad_fn=&lt;NllLossBackward0&gt;)
3900 tensor(2.7886, grad_fn=&lt;NllLossBackward0&gt;)
4000 tensor(2.7494, grad_fn=&lt;NllLossBackward0&gt;)
4100 tensor(2.4470, grad_fn=&lt;NllLossBackward0&gt;)
4200 tensor(3.0389, grad_fn=&lt;NllLossBackward0&gt;)
4300 tensor(2.5901, grad_fn=&lt;NllLossBackward0&gt;)
4400 tensor(2.6222, grad_fn=&lt;NllLossBackward0&gt;)
4500 tensor(2.7454, grad_fn=&lt;NllLossBackward0&gt;)
4600 tensor(2.5700, grad_fn=&lt;NllLossBackward0&gt;)
4700 tensor(2.7499, grad_fn=&lt;NllLossBackward0&gt;)
4800 tensor(2.3950, grad_fn=&lt;NllLossBackward0&gt;)
4900 tensor(2.7216, grad_fn=&lt;NllLossBackward0&gt;)
5000 tensor(2.5265, grad_fn=&lt;NllLossBackward0&gt;)
5100 tensor(2.8514, grad_fn=&lt;NllLossBackward0&gt;)
5200 tensor(2.6356, grad_fn=&lt;NllLossBackward0&gt;)
5300 tensor(2.6787, grad_fn=&lt;NllLossBackward0&gt;)
5400 tensor(2.1865, grad_fn=&lt;NllLossBackward0&gt;)
5500 tensor(2.9881, grad_fn=&lt;NllLossBackward0&gt;)
5600 tensor(2.4254, grad_fn=&lt;NllLossBackward0&gt;)
5700 tensor(2.1786, grad_fn=&lt;NllLossBackward0&gt;)
5800 tensor(2.5196, grad_fn=&lt;NllLossBackward0&gt;)
5900 tensor(2.7930, grad_fn=&lt;NllLossBackward0&gt;)
6000 tensor(3.1752, grad_fn=&lt;NllLossBackward0&gt;)
6100 tensor(2.9611, grad_fn=&lt;NllLossBackward0&gt;)
6200 tensor(2.0737, grad_fn=&lt;NllLossBackward0&gt;)
6300 tensor(3.2208, grad_fn=&lt;NllLossBackward0&gt;)
6400 tensor(2.0019, grad_fn=&lt;NllLossBackward0&gt;)
6500 tensor(2.4998, grad_fn=&lt;NllLossBackward0&gt;)
6600 tensor(2.3953, grad_fn=&lt;NllLossBackward0&gt;)
6700 tensor(2.6234, grad_fn=&lt;NllLossBackward0&gt;)
6800 tensor(2.6536, grad_fn=&lt;NllLossBackward0&gt;)
6900 tensor(3.0955, grad_fn=&lt;NllLossBackward0&gt;)
7000 tensor(2.5247, grad_fn=&lt;NllLossBackward0&gt;)
7100 tensor(2.1732, grad_fn=&lt;NllLossBackward0&gt;)
7200 tensor(3.0029, grad_fn=&lt;NllLossBackward0&gt;)
7300 tensor(2.4774, grad_fn=&lt;NllLossBackward0&gt;)
7400 tensor(2.4172, grad_fn=&lt;NllLossBackward0&gt;)
7500 tensor(2.2456, grad_fn=&lt;NllLossBackward0&gt;)
7600 tensor(2.2593, grad_fn=&lt;NllLossBackward0&gt;)
7700 tensor(2.7350, grad_fn=&lt;NllLossBackward0&gt;)
7800 tensor(2.2286, grad_fn=&lt;NllLossBackward0&gt;)
7900 tensor(2.0104, grad_fn=&lt;NllLossBackward0&gt;)
8000 tensor(2.3917, grad_fn=&lt;NllLossBackward0&gt;)
8100 tensor(2.4749, grad_fn=&lt;NllLossBackward0&gt;)
8200 tensor(2.3486, grad_fn=&lt;NllLossBackward0&gt;)
8300 tensor(2.0631, grad_fn=&lt;NllLossBackward0&gt;)
8400 tensor(2.6369, grad_fn=&lt;NllLossBackward0&gt;)
8500 tensor(2.2690, grad_fn=&lt;NllLossBackward0&gt;)
8600 tensor(2.3299, grad_fn=&lt;NllLossBackward0&gt;)
8700 tensor(2.4710, grad_fn=&lt;NllLossBackward0&gt;)
8800 tensor(2.8632, grad_fn=&lt;NllLossBackward0&gt;)
8900 tensor(2.0429, grad_fn=&lt;NllLossBackward0&gt;)
9000 tensor(1.9595, grad_fn=&lt;NllLossBackward0&gt;)
9100 tensor(3.0452, grad_fn=&lt;NllLossBackward0&gt;)
9200 tensor(2.2445, grad_fn=&lt;NllLossBackward0&gt;)
9300 tensor(2.1877, grad_fn=&lt;NllLossBackward0&gt;)
9400 tensor(2.4496, grad_fn=&lt;NllLossBackward0&gt;)
9500 tensor(2.6318, grad_fn=&lt;NllLossBackward0&gt;)
9600 tensor(2.3513, grad_fn=&lt;NllLossBackward0&gt;)
9700 tensor(2.4380, grad_fn=&lt;NllLossBackward0&gt;)
9800 tensor(2.5171, grad_fn=&lt;NllLossBackward0&gt;)
9900 tensor(2.4787, grad_fn=&lt;NllLossBackward0&gt;)
9999 tensor(3.1696, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-208" class="cell">
<div class="sourceCode cell-code" id="cb309"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb309-1"><a href="#cb309-1" aria-hidden="true" tabindex="-1"></a>train(model, Xtr, Ytr, lr<span class="op">=</span><span class="fl">0.011</span>, epochs<span class="op">=</span><span class="dv">10000</span>, batch_sz<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>, tracker<span class="op">=</span>tracker)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 tensor(2.4361, grad_fn=&lt;NllLossBackward0&gt;)
100 tensor(2.5009, grad_fn=&lt;NllLossBackward0&gt;)
200 tensor(1.6655, grad_fn=&lt;NllLossBackward0&gt;)
300 tensor(1.7574, grad_fn=&lt;NllLossBackward0&gt;)
400 tensor(2.5683, grad_fn=&lt;NllLossBackward0&gt;)
500 tensor(1.8705, grad_fn=&lt;NllLossBackward0&gt;)
600 tensor(2.2056, grad_fn=&lt;NllLossBackward0&gt;)
700 tensor(1.9754, grad_fn=&lt;NllLossBackward0&gt;)
800 tensor(1.9142, grad_fn=&lt;NllLossBackward0&gt;)
900 tensor(2.0410, grad_fn=&lt;NllLossBackward0&gt;)
1000 tensor(2.1931, grad_fn=&lt;NllLossBackward0&gt;)
1100 tensor(2.0752, grad_fn=&lt;NllLossBackward0&gt;)
1200 tensor(2.0992, grad_fn=&lt;NllLossBackward0&gt;)
1300 tensor(2.6865, grad_fn=&lt;NllLossBackward0&gt;)
1400 tensor(2.5488, grad_fn=&lt;NllLossBackward0&gt;)
1500 tensor(1.9268, grad_fn=&lt;NllLossBackward0&gt;)
1600 tensor(1.7528, grad_fn=&lt;NllLossBackward0&gt;)
1700 tensor(2.1228, grad_fn=&lt;NllLossBackward0&gt;)
1800 tensor(2.3399, grad_fn=&lt;NllLossBackward0&gt;)
1900 tensor(2.0663, grad_fn=&lt;NllLossBackward0&gt;)
2000 tensor(2.2824, grad_fn=&lt;NllLossBackward0&gt;)
2100 tensor(2.8879, grad_fn=&lt;NllLossBackward0&gt;)
2200 tensor(2.7201, grad_fn=&lt;NllLossBackward0&gt;)
2300 tensor(1.8944, grad_fn=&lt;NllLossBackward0&gt;)
2400 tensor(2.2795, grad_fn=&lt;NllLossBackward0&gt;)
2500 tensor(1.7492, grad_fn=&lt;NllLossBackward0&gt;)
2600 tensor(2.3349, grad_fn=&lt;NllLossBackward0&gt;)
2700 tensor(1.9319, grad_fn=&lt;NllLossBackward0&gt;)
2800 tensor(2.3628, grad_fn=&lt;NllLossBackward0&gt;)
2900 tensor(2.6416, grad_fn=&lt;NllLossBackward0&gt;)
3000 tensor(3.0241, grad_fn=&lt;NllLossBackward0&gt;)
3100 tensor(2.1092, grad_fn=&lt;NllLossBackward0&gt;)
3200 tensor(2.3266, grad_fn=&lt;NllLossBackward0&gt;)
3300 tensor(2.3129, grad_fn=&lt;NllLossBackward0&gt;)
3400 tensor(2.3494, grad_fn=&lt;NllLossBackward0&gt;)
3500 tensor(2.4274, grad_fn=&lt;NllLossBackward0&gt;)
3600 tensor(2.3356, grad_fn=&lt;NllLossBackward0&gt;)
3700 tensor(3.1555, grad_fn=&lt;NllLossBackward0&gt;)
3800 tensor(2.6469, grad_fn=&lt;NllLossBackward0&gt;)
3900 tensor(2.1959, grad_fn=&lt;NllLossBackward0&gt;)
4000 tensor(2.4170, grad_fn=&lt;NllLossBackward0&gt;)
4100 tensor(2.1809, grad_fn=&lt;NllLossBackward0&gt;)
4200 tensor(1.6268, grad_fn=&lt;NllLossBackward0&gt;)
4300 tensor(2.0488, grad_fn=&lt;NllLossBackward0&gt;)
4400 tensor(2.1533, grad_fn=&lt;NllLossBackward0&gt;)
4500 tensor(1.8331, grad_fn=&lt;NllLossBackward0&gt;)
4600 tensor(1.9599, grad_fn=&lt;NllLossBackward0&gt;)
4700 tensor(2.5073, grad_fn=&lt;NllLossBackward0&gt;)
4800 tensor(2.1599, grad_fn=&lt;NllLossBackward0&gt;)
4900 tensor(1.9360, grad_fn=&lt;NllLossBackward0&gt;)
5000 tensor(2.2972, grad_fn=&lt;NllLossBackward0&gt;)
5100 tensor(2.1643, grad_fn=&lt;NllLossBackward0&gt;)
5200 tensor(1.9733, grad_fn=&lt;NllLossBackward0&gt;)
5300 tensor(2.0075, grad_fn=&lt;NllLossBackward0&gt;)
5400 tensor(1.9952, grad_fn=&lt;NllLossBackward0&gt;)
5500 tensor(2.4494, grad_fn=&lt;NllLossBackward0&gt;)
5600 tensor(2.4025, grad_fn=&lt;NllLossBackward0&gt;)
5700 tensor(2.7972, grad_fn=&lt;NllLossBackward0&gt;)
5800 tensor(1.9964, grad_fn=&lt;NllLossBackward0&gt;)
5900 tensor(2.3996, grad_fn=&lt;NllLossBackward0&gt;)
6000 tensor(2.2986, grad_fn=&lt;NllLossBackward0&gt;)
6100 tensor(2.0393, grad_fn=&lt;NllLossBackward0&gt;)
6200 tensor(2.1373, grad_fn=&lt;NllLossBackward0&gt;)
6300 tensor(2.1929, grad_fn=&lt;NllLossBackward0&gt;)
6400 tensor(2.2594, grad_fn=&lt;NllLossBackward0&gt;)
6500 tensor(2.2243, grad_fn=&lt;NllLossBackward0&gt;)
6600 tensor(2.4098, grad_fn=&lt;NllLossBackward0&gt;)
6700 tensor(1.9735, grad_fn=&lt;NllLossBackward0&gt;)
6800 tensor(2.0293, grad_fn=&lt;NllLossBackward0&gt;)
6900 tensor(2.2012, grad_fn=&lt;NllLossBackward0&gt;)
7000 tensor(2.5856, grad_fn=&lt;NllLossBackward0&gt;)
7100 tensor(2.0930, grad_fn=&lt;NllLossBackward0&gt;)
7200 tensor(2.7044, grad_fn=&lt;NllLossBackward0&gt;)
7300 tensor(2.5436, grad_fn=&lt;NllLossBackward0&gt;)
7400 tensor(2.2447, grad_fn=&lt;NllLossBackward0&gt;)
7500 tensor(2.4728, grad_fn=&lt;NllLossBackward0&gt;)
7600 tensor(2.1327, grad_fn=&lt;NllLossBackward0&gt;)
7700 tensor(2.0702, grad_fn=&lt;NllLossBackward0&gt;)
7800 tensor(2.3176, grad_fn=&lt;NllLossBackward0&gt;)
7900 tensor(2.1807, grad_fn=&lt;NllLossBackward0&gt;)
8000 tensor(2.1489, grad_fn=&lt;NllLossBackward0&gt;)
8100 tensor(2.0514, grad_fn=&lt;NllLossBackward0&gt;)
8200 tensor(1.8035, grad_fn=&lt;NllLossBackward0&gt;)
8300 tensor(1.8368, grad_fn=&lt;NllLossBackward0&gt;)
8400 tensor(2.6862, grad_fn=&lt;NllLossBackward0&gt;)
8500 tensor(2.0700, grad_fn=&lt;NllLossBackward0&gt;)
8600 tensor(2.3183, grad_fn=&lt;NllLossBackward0&gt;)
8700 tensor(2.2437, grad_fn=&lt;NllLossBackward0&gt;)
8800 tensor(2.5779, grad_fn=&lt;NllLossBackward0&gt;)
8900 tensor(3.0711, grad_fn=&lt;NllLossBackward0&gt;)
9000 tensor(1.9861, grad_fn=&lt;NllLossBackward0&gt;)
9100 tensor(2.1829, grad_fn=&lt;NllLossBackward0&gt;)
9200 tensor(2.2452, grad_fn=&lt;NllLossBackward0&gt;)
9300 tensor(2.4202, grad_fn=&lt;NllLossBackward0&gt;)
9400 tensor(2.3188, grad_fn=&lt;NllLossBackward0&gt;)
9500 tensor(2.3009, grad_fn=&lt;NllLossBackward0&gt;)
9600 tensor(2.5345, grad_fn=&lt;NllLossBackward0&gt;)
9700 tensor(1.8759, grad_fn=&lt;NllLossBackward0&gt;)
9800 tensor(2.2283, grad_fn=&lt;NllLossBackward0&gt;)
9900 tensor(2.2164, grad_fn=&lt;NllLossBackward0&gt;)
9999 tensor(1.9654, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.Model&gt;</code></pre>
</div>
</div>
<div id="cell-209" class="cell">
<div class="sourceCode cell-code" id="cb312"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb312-1"><a href="#cb312-1" aria-hidden="true" tabindex="-1"></a>F.cross_entropy(model(Xdev), Ydev)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(2.5698, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-210" class="cell">
<div class="sourceCode cell-code" id="cb314"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb314-1"><a href="#cb314-1" aria-hidden="true" tabindex="-1"></a>model.C.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([27, 10])</code></pre>
</div>
</div>
<div id="cell-211" class="cell">
<div class="sourceCode cell-code" id="cb316"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb316-1"><a href="#cb316-1" aria-hidden="true" tabindex="-1"></a>emb_model <span class="op">=</span> TSNE(random_state<span class="op">=</span><span class="dv">0</span>, n_iter<span class="op">=</span><span class="dv">1000</span>, perplexity<span class="op">=</span><span class="dv">26</span>)</span>
<span id="cb316-2"><a href="#cb316-2" aria-hidden="true" tabindex="-1"></a><span class="co"># emb_model.fit_transform(model.C.item())</span></span>
<span id="cb316-3"><a href="#cb316-3" aria-hidden="true" tabindex="-1"></a><span class="co"># emb_model.fit_transform(model.C.detach().numpy())</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-212" class="cell">
<div class="sourceCode cell-code" id="cb317"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb317-1"><a href="#cb317-1" aria-hidden="true" tabindex="-1"></a>plot_embeddings(model, s2i, emb_model <span class="op">=</span> emb_model, cluster_model<span class="op">=</span>KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>)), <span class="op">;</span> F.cross_entropy(model(X), Y) , model.num_params(), model.hidden_units</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/homebrew/Caskroom/miniforge/base/envs/aiking/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-167-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-213" class="cell">
<div class="sourceCode cell-code" id="cb319"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb319-1"><a href="#cb319-1" aria-hidden="true" tabindex="-1"></a>plt.plot(tracker[<span class="st">'loss'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-168-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-214" class="cell">
<div class="sourceCode cell-code" id="cb320"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb320-1"><a href="#cb320-1" aria-hidden="true" tabindex="-1"></a>lrs <span class="op">=</span> torch.tensor(tracker[<span class="st">'lr'</span>])</span>
<span id="cb320-2"><a href="#cb320-2" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> torch.tensor(tracker[<span class="st">'loss'</span>])</span>
<span id="cb320-3"><a href="#cb320-3" aria-hidden="true" tabindex="-1"></a>log_lrs <span class="op">=</span> lrs.log()</span>
<span id="cb320-4"><a href="#cb320-4" aria-hidden="true" tabindex="-1"></a>min_idx <span class="op">=</span> losses.<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).indices.item()</span>
<span id="cb320-5"><a href="#cb320-5" aria-hidden="true" tabindex="-1"></a>plt.plot(log_lrs, tracker[<span class="st">'loss'</span>])</span>
<span id="cb320-6"><a href="#cb320-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(log_lrs[min_idx], color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb320-7"><a href="#cb320-7" aria-hidden="true" tabindex="-1"></a>plt.text(log_lrs[min_idx]<span class="op">*</span><span class="fl">0.9</span>,losses[min_idx]<span class="op">*</span><span class="fl">0.9</span>, <span class="ss">f"lr : </span><span class="sc">{</span>lrs[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">, loss : </span><span class="sc">{</span>losses[min_idx]<span class="sc">.</span>item()<span class="sc">:0.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(tensor(-4.0589), tensor(1.2932), 'lr : 0.01, loss : 1.44')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-169-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-215" class="cell">
<div class="sourceCode cell-code" id="cb322"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb322-1"><a href="#cb322-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.plot(tracker[<span class="st">'loss'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-170-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-216" class="cell">
<div class="sourceCode cell-code" id="cb323"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb323-1"><a href="#cb323-1" aria-hidden="true" tabindex="-1"></a>plt.plot(tracker[<span class="st">'lr'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_makemore.mlp_files/figure-html/cell-171-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="additional-hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="additional-hyperparameter-tuning">Additional Hyperparameter Tuning</h3>
<ul>
<li><p>Batch size is so low we have , that we have way too much noise in the training –&gt; increase batch size , so we have correct gradient and we are not jumping up and down on the gradient. And we can optimize more properly</p></li>
<li><p><del>May be our embedding size if low . We are cramming way too much neurons in 2 dimension and neural network is not able to use the space effectively</del></p></li>
</ul>
</section>
</section>
<section id="todo" class="level2">
<h2 class="anchored" data-anchor-id="todo">Todo</h2>
<ul>
<li>Additional hyperparameter tuning - “check for better convergence with batch size changes”, increasing embedding size</li>
<li>Notes on last few minutes of video</li>
<li><del><code>F.softmax</code> in word generation</del></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/Rahuketu86\.github\.io\/minion");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/Rahuketu86/minion/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>