# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/04_makemore.mlp.ipynb.

# %% auto 0
__all__ = ['build_XY', 'Model', 'softmax', 'nll', 'plot_embeddings', 'train', 'gen_word_nn', 'lr_scheduler']

# %% ../../nbs/04_makemore.mlp.ipynb 3
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from .bigram import stoi, itos
import math
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans

# %% ../../nbs/04_makemore.mlp.ipynb 9
def build_XY(words, s2i, block_size, str_term=".", verbose=False):
    X, Y = [], []
    for w in words:
        context = [s2i[str_term]]*block_size  # We need numerical embedding for each character 0 is for "."
        for ch in w + str_term:
            if verbose: 
                i2s = itos(s2i)
                print("".join([i2s[i] for i in context]),"-->", ch)
            X.append(context)
            Y.append(s2i[ch])
            context = context[1:] + [s2i[ch]]
    return torch.tensor(X), torch.tensor(Y)

# %% ../../nbs/04_makemore.mlp.ipynb 68
class Model(object):
    def __init__(self, vocab_sz, blck_sz=3, emb_sz=2, hidden_units=100, g=torch.Generator().manual_seed(2147483647)) -> None:
        self.C = torch.randn((vocab_sz,emb_sz), generator=g, requires_grad=True)
        self.W1 = torch.randn((blck_sz*emb_sz, hidden_units), generator=g, requires_grad=True)
        self.b1 = torch.randn(hidden_units, generator=g, requires_grad=True)
        self.W2 = torch.randn((hidden_units, vocab_sz), generator=g, requires_grad=True)
        self.b2 = torch.randn(vocab_sz, generator=g, requires_grad=True)
        self.blck_sz = blck_sz
        self.emb_sz = emb_sz
        self.hidden_units = hidden_units

    def __call__(self, X):
        emb = self.C[X]
        h = torch.tanh(emb.view(-1, emb.shape[1]*emb.shape[2])@self.W1 + self.b1)
        logits = h@self.W2 + self.b2
        return logits
    
    def parameters(self):
        return [self.C, self.W1, self.b1, self.W2, self.b2]

    def num_params(self):
        return sum(p.nelement() for p in self.parameters())
    
    def zero_grad(self):
        for p in self.parameters(): 
            # print(p.shape, p.data, p.grad)
            p.grad = None
    

# %% ../../nbs/04_makemore.mlp.ipynb 71
def softmax(inputs, dim=1):
    c  = -torch.max(inputs)
    counts = (inputs+c).exp()
    probs = counts/ counts.sum(dim=dim, keepdim=True)
    return probs

# %% ../../nbs/04_makemore.mlp.ipynb 75
def nll(inputs,  #Takes logits
        target  #Takes y
        ): 
    # counts = inputs.exp()
    # c  = -torch.max(inputs)
    # counts = (inputs+c).exp()
    # probs = counts/ counts.sum(dim=1, keepdim=True)
    probs = softmax(inputs, dim=1)
    loss = -probs[torch.arange(len(target)), target].log().mean()
    return loss


# %% ../../nbs/04_makemore.mlp.ipynb 83
def plot_embeddings(model, s2i, emb_model=None, cluster_model = None):
    i2s = itos(s2i)
    plt.figure(figsize=(8,8))
    
    c = model.C.detach().numpy()
    if emb_model:
        c = emb_model.fit_transform(a)
    if not cluster_model: plt.scatter(a[:,0], a[:,1], s=200)
    else: 
        label = cluster_model.fit_predict(c)
        plt.scatter(a[:,0], a[:,1], s=200, c=label)
    for i in range(a.shape[0]):
        # plt.text()
        plt.text(a[i,0], a[i,1], i2s[i], ha='center', va='center', color='white')
    plt.grid()

# %% ../../nbs/04_makemore.mlp.ipynb 88
def train(model, X, Y, lr=0.1, epochs=1000, verbose=False, batch_sz=None, loss_fn=F.cross_entropy, tracker = None):
    

    for i in range(epochs):
    
        #minibatch construct
        inputs = X
        target = Y
        if batch_sz:
            ix = torch.randint(low=0, high=X.shape[0], size=(batch_sz,))
            inputs = X[ix]
            target = Y[ix]
        
        # Forward Pass
        logits = model(inputs); logits.shape 
        # loss = nll(logits, Y)
        loss = loss_fn(logits, target)
        # loss2 = nll(logits, Y)

        # Backward Pass
        model.zero_grad()
        loss.backward(); 
        if i%100 == 0 and verbose: print(i, loss)
        if i == epochs-1: print(i, loss)


        ## Update / Gradient
        lri = next(lr).item() if hasattr(lr, "__next__") else lr
        for p in model.parameters():
            p.data -=lri*p.grad

        if tracker is not None:
            tracker.get('lr', []).append(lri)
            tracker.get('loss', []).append(loss.item())
            tracker.get('batch_sz', []).append(batch_sz)
            tracker.get('block_sz', []).append(X.shape[1])
            tracker.get('emb_sz', []).append(model.C.shape[1])
            tracker.get('hidden_units', []).append(model.b1.shape[0])
    return model


# %% ../../nbs/04_makemore.mlp.ipynb 112
def gen_word_nn(model, i2s, n_samples=20, g=torch.Generator().manual_seed(2147483647), logit2prob=F.softmax):
    gen_words = []
    for i in range(n_samples):
        ix = 0
        gen_word = ""
        inp = [ix]*model.blck_sz
        
        while True:
            logits = model(torch.tensor([inp]))
            # counts = logits.exp() # equivalent N
            # probs = counts/ counts.sum(1, keepdims=True)

            probs = logit2prob(logits, dim=1)
            ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()
            
            inp.pop(0)
            inp.append(ix)
            if ix == 0: break
            else: gen_word +=i2s[ix]
        gen_words.append(gen_word)
    return gen_words

# %% ../../nbs/04_makemore.mlp.ipynb 123
def lr_scheduler(low=-3, upper=0, steps=1000):
    yield from 10**torch.linspace(low, upper, steps)

# for i in lr_scheduler():
#     print(i)
    # break

# torch.linspace(-3, 0, 1000)

# for i in torch.pow(10, torch.linspace(-3, 0, 1000)):
#     print(i)
# for i in torch.pow(10, torch.linspace(-3, 0, 1000)):
#     print(i.item())


