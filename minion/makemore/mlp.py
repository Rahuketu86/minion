# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/04_makemore.mlp.ipynb.

# %% auto 0
__all__ = ['build_XY', 'Model', 'nll', 'plot_embeddings', 'train', 'gen_word_nn']

# %% ../../nbs/04_makemore.mlp.ipynb 3
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from .bigram import stoi, itos

# %% ../../nbs/04_makemore.mlp.ipynb 9
def build_XY(words, s2i, block_size, str_term=".", verbose=False):
    X, Y = [], []
    for w in words:
        context = [s2i[str_term]]*block_size  # We need numerical embedding for each character 0 is for "."
        for ch in w + str_term:
            if verbose: 
                i2s = itos(s2i)
                print("".join([i2s[i] for i in context]),"-->", ch)
            X.append(context)
            Y.append(s2i[ch])
            context = context[1:] + [s2i[ch]]
    return torch.tensor(X), torch.tensor(Y)

# %% ../../nbs/04_makemore.mlp.ipynb 67
class Model(object):
    def __init__(self, s2i, blck_sz=3, emb_sz=2, hidden_units=100, g=torch.Generator().manual_seed(2147483647)) -> None:
        self.C = torch.randn((len(s2i),emb_sz), generator=g, requires_grad=True)
        self.W1 = torch.randn((blck_sz*emb_sz, hidden_units), generator=g, requires_grad=True)
        self.b1 = torch.randn(hidden_units, generator=g, requires_grad=True)
        self.W2 = torch.randn((hidden_units, len(s2i)), generator=g, requires_grad=True)
        self.b2 = torch.randn(len(s2i), generator=g, requires_grad=True)
        self.blck_sz = blck_sz
        self.emb_sz = emb_sz
        self.hidden_units = hidden_units

    def __call__(self, X):
        emb = self.C[X]
        h = torch.tanh(emb.view(-1, emb.shape[1]*emb.shape[2])@self.W1 + self.b1)
        logits = h@self.W2 + self.b2
        return logits
    
    def parameters(self):
        return [self.C, self.W1, self.b1, self.W2, self.b2]
    
    def zero_grad(self):
        for p in self.parameters(): 
            # print(p.shape, p.data, p.grad)
            p.grad = None
    

# %% ../../nbs/04_makemore.mlp.ipynb 70
def nll(inputs,  #Takes logits
        target  #Takes y
        ): 
    counts = inputs.exp()
    probs = counts/ counts.sum(dim=1, keepdim=True)
    loss = -probs[torch.arange(len(target)), target].log().mean()
    return loss



# %% ../../nbs/04_makemore.mlp.ipynb 76
def plot_embeddings(model, s2i):
    i2s = itos(s2i)
    plt.figure(figsize=(8,8))
    plt.scatter(model.C[:,0].data, model.C[:,1].data, s=200)
    for i in range(model.C.shape[0]):
        # plt.text()
        plt.text(model.C[i,0].item(), model.C[i,1].item(), i2s[i], ha='center', va='center', color='white')
    plt.grid()

# %% ../../nbs/04_makemore.mlp.ipynb 79
def train(model, X, Y, lr=0.1, epochs=1000, verbose=False, batch_sz=None, loss_fn=F.cross_entropy, tracker = None):
    for i in range(epochs):
    
        #minibatch construct
        inputs = X
        target = Y
        if batch_sz:
            ix = torch.randint(low=0, high=X.shape[0], size=(batch_sz,))
            inputs = X[ix]
            target = Y[ix]
        
        # Forward Pass
        logits = model(inputs); logits.shape 
        # loss = nll(logits, Y)
        loss = loss_fn(logits, target)
        # loss2 = nll(logits, Y)

        # Backward Pass
        model.zero_grad()
        loss.backward(); 
        if i%100 == 0 and verbose: print(i, loss)
        if i == epochs-1: print(i, loss)


        ## Update / Gradient 
        for p in model.parameters():
            p.data -=lr*p.grad

        if tracker is not None:
            tracker.get('lr', []).append(lr)
            tracker.get('loss', []).append(loss.item())
            tracker.get('batch_sz', []).append(batch_sz)
            tracker.get('block_sz', []).append(X.shape[1])
            tracker.get('emb_sz', []).append(model.C.shape[1])
            tracker.get('hidden_units', []).append(model.b1.shape[0])
    return model


# %% ../../nbs/04_makemore.mlp.ipynb 101
def gen_word_nn(model, i2s, n_samples=20, g=torch.Generator().manual_seed(2147483647)):
    gen_words = []
    for i in range(n_samples):
        ix = 0
        gen_word = ""
        inp = [ix]*model.blck_sz
        
        while True:
            logits = model(torch.tensor([inp]))
            counts = logits.exp() # equivalent N
            probs = counts/ counts.sum(1, keepdims=True)
            ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()
            
            inp.pop(0)
            inp.append(ix)
            if ix == 0: break
            else: gen_word +=i2s[ix]
        gen_words.append(gen_word)
    return gen_words
