# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/04_makemore.mlp.ipynb.

# %% auto 0
__all__ = ['build_XY', 'Model', 'nll', 'train']

# %% ../../nbs/04_makemore.mlp.ipynb 3
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from .bigram import stoi, itos

# %% ../../nbs/04_makemore.mlp.ipynb 9
def build_XY(words, s2i, block_size, str_term=".", verbose=False):
    X, Y = [], []
    for w in words:
        context = [s2i[str_term]]*block_size  # We need numerical embedding for each character 0 is for "."
        for ch in w + str_term:
            if verbose: 
                i2s = itos(s2i)
                print("".join([i2s[i] for i in context]),"-->", ch)
            X.append(context)
            Y.append(s2i[ch])
            context = context[1:] + [s2i[ch]]
    return torch.tensor(X), torch.tensor(Y)

# %% ../../nbs/04_makemore.mlp.ipynb 66
class Model(object):
    def __init__(self, s2i, blck_sz=3, emb_sz=2, hidden_units=100, g=torch.Generator().manual_seed(2147483647)) -> None:
        self.C = torch.randn((len(s2i),emb_sz), generator=g, requires_grad=True)
        self.W1 = torch.randn((blck_sz*emb_sz, hidden_units), generator=g, requires_grad=True)
        self.b1 = torch.randn(hidden_units, generator=g, requires_grad=True)
        self.W2 = torch.randn((hidden_units, len(s2i)), generator=g, requires_grad=True)
        self.b2 = torch.randn(len(s2i), generator=g, requires_grad=True)

    def __call__(self, X):
        emb = self.C[X]
        h = torch.tanh(emb.view(-1, emb.shape[1]*emb.shape[2])@self.W1 + self.b1)
        logits = h@self.W2 + self.b2
        return logits
    
    def parameters(self):
        return [self.C, self.W1, self.b1, self.W2, self.b2]
    
    def zero_grad(self):
        for p in self.parameters(): 
            # print(p.shape, p.data, p.grad)
            p.grad = None
    

# %% ../../nbs/04_makemore.mlp.ipynb 68
def nll(input,  #Takes logits
        target  #Takes y
        ): 
    counts = logits.exp()
    probs = counts/ counts.sum(dim=1, keepdim=True)
    loss = -probs[torch.arange(len(Y)), Y].log().mean()
    return loss



# %% ../../nbs/04_makemore.mlp.ipynb 70
def train(model, X, Y, lr=0.1, epochs=1000, verbose=False, loss_fn=F.cross_entropy):
    for i in range(epochs):
        # model = Model(s2i)

        # Forward Pass
        logits = model(X); logits.shape 
        # loss = nll(logits, Y)
        loss = loss_fn(logits, Y)
        # loss2 = nll(logits, Y)

        # Backward Pass
        model.zero_grad()
        loss.backward(); 
        if i%100 == 0 and verbose: print(i, loss)
        if i == epochs: print(i, loss)

        # # Update
        # for p in model.parameters():
        #     p.data -= lr*p.grad

        # model.parameters()[0].grad

        # model.parameters()[1].grad
        # len(model.parameters())

        for p in model.parameters():
            p.data -=lr*p.grad
    return model

